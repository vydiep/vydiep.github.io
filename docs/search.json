[
  {
    "objectID": "posts/example-blog-post/index.html",
    "href": "posts/example-blog-post/index.html",
    "title": "Hello Blog",
    "section": "",
    "text": "This is an example of the blog posts that you’ll submit as your primary form of learning demonstration in CSCI 0451. I created this post by modifying the file posts/example-blog-post/index.ipynb in VSCode. You can also use JupyterLab for this editing if you prefer. Finally, it is possible to write blog posts without using notebooks by writing .qmd files, as illustrated here."
  },
  {
    "objectID": "posts/example-blog-post/index.html#math",
    "href": "posts/example-blog-post/index.html#math",
    "title": "Hello Blog",
    "section": "Math",
    "text": "Math\nIn addition to regular text using the Markdown specification, you can also write mathematics, enclosed between dollar signs. The syntax for writing math is very similar to the syntax used in the \\(\\LaTeX\\) markup language. For example, $f(x) \\approx y$ renders to \\(f(x) \\approx y\\). To place complex mathematical expressions on their own lines, use double dollar signs. For example, the expression\n$$\\mathcal{L}(a, b) = \\sum_{i = 1}^n (ax_i + b - y_i)^2$$\nrenders to:\n\\[\\mathcal{L}(a, b) = \\sum_{i = 1}^n (ax_i + b - y_i)^2\\;.\\]\nBehind the scenes, math is powered by the MathJax engine. For more on how to write math, check this handy tutorial and quick reference."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "My Awesome CSCI 0451 Blog",
    "section": "",
    "text": "An example blog post illustrating the key techniques you’ll need to demonstrate your learning in CSCI 0451.\n\n\n\n\n\n\nJan 10, 2023\n\n\nPhil Chodrow\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "About this blog"
  },
  {
    "objectID": "posts/perceptron/perceptron.html",
    "href": "posts/perceptron/perceptron.html",
    "title": "My Awesome CSCI 0451 Blog",
    "section": "",
    "text": "Source code: perceptron.py\n\n\n\nThis blog post will cover the perceptron algorithm. This is a binary classification algorithm that takes in an input and finds a hyperplane that splits that input data such that all similar data points lie on either side of the hyperplane. This algorithm only works if the data is linearly separable.\n\n\n\n\nIn our Perceptron class, we implemented a fit(X, y) method that takes in an array of some data, X, and an array of labels, y, corresponding to the elements in the input data. Each element in y is either a 1 or a 0.\nIn the method, we started with a random value for weight, self.w, such that w = (w, -b). We also initalized a list, self.history, that would keep track of the scores over the training period. While the classification is not perfect (i.e., achieved a 100% accuracy rate) and the max number of iterations has not been reached, we will continue to update our w depending on if it was able to correctly classify a random data point, i, in our data set X. If it wasn’t able to correctly classify the random data point, we update w using the following equation:\n\nThis update is done in line 52 of our perceptron.py code. This update moves our separator in the right direction so that our randomly selected point could be correctly classified on the next iteration if it were to be selected again. As this loops continues, the weight is added to self.history.\n\n\n\n\nBefore starting, you need to load all of these libraries and modules:\n\nimport numpy as np\nimport pandas as pd\nimport seaborn as sns\nfrom matplotlib import pyplot as plt\nfrom perceptron import Perceptron\nfrom sklearn.datasets import make_blobs\n\n\n\n\nThis block of code generates a visualization that contains data that is linearly separable with 2 features.\n\nnp.random.seed(12345)\n\nn = 100\np_features = 3\n\nX, y = make_blobs(n_samples = 100, n_features = p_features - 1, centers = [(-1.7, -1.7), (1.7, 1.7)])\n\nfig = plt.scatter(X[:,0], X[:,1], c = y)\nxlab = plt.xlabel(\"Feature 1\")\nylab = plt.ylabel(\"Feature 2\")\n\n\n\n\nJust by looking at the graph, we can see that it is possible to have a line that separates all the purple “0” labels from the yellow “1” labels. To analyze it further, however, we can use the algorithm we implemented!\n\np = Perceptron()\np.fit(X, y, max_steps = 1000)\nfig = plt.plot(p.history)\nxlab = plt.xlabel(\"Iteration\")\nylab = plt.ylabel(\"Accuracy\")\n\n\n\n\nFrom the graph above, we can see that while there are variations in the accuracy, the algorithm does eventually achieved a perfect classification rate of 1.00. This means that the data was indeed separable and we were able to separate the data points into their respective labels.\n\ndef draw_line(w, x_min, x_max):\n  x = np.linspace(x_min, x_max, 101)\n  y = -(w[0]*x + w[2])/w[1]\n  plt.plot(x, y, color = \"black\")\n\nfig = plt.scatter(X[:,0], X[:,1], c = y)\nfig = draw_line(p.w, -2, 2)\n\nxlab = plt.xlabel(\"Feature 1\")\nylab = plt.ylabel(\"Feature 2\")\n\n\n\n\nHere is a visualization of the graph with a line separating the data points.\n\n\n\n\nThis block of code generates a visualization that contains data that is not linearly separable with 2 features.\n\nnp.random.seed(12345)\n\nn = 100\np_features = 3\n\nX, y = make_blobs(n_samples = 100, n_features = p_features - 1, centers = [(-1, -1), (1, 1)])\n\nfig = plt.scatter(X[:,0], X[:,1], c = y)\nxlab = plt.xlabel(\"Feature 1\")\nylab = plt.ylabel(\"Feature 2\")\n\n\n\n\n\np = Perceptron()\np.fit(X, y, max_steps = 1000)\nfig = plt.plot(p.history)\nxlab = plt.xlabel(\"Iteration\")\nylab = plt.ylabel(\"Accuracy\")\n\n\n\n\nHere we see the oscillation of accuracy. It never reaches 1.0 and the iterating stops once the max number of iterations is reached. This means the data is not linearly separable and the algorithm was not able to draw a line that would be able to separate the data points.\n\nfig = plt.scatter(X[:,0], X[:,1], c = y)\nfig = draw_line(p.w, -2, 2)\n\nxlab = plt.xlabel(\"Feature 1\")\nylab = plt.ylabel(\"Feature 2\")\n\n\n\n\nThis visualization shows that the algorithm was not able to find a line that would separate the data points accordingly.\n\n\n\n\nBelow is a block of code that generations a visualization of the evolution of the score over the training period of a random data set that contains 7 features.\n\nnp.random.seed(12345)\n\nn = 100\np_features = 8\n\nX, y = make_blobs(n_samples = 100, n_features = p_features - 1, centers = [(0, 0), (0, 0)])\n\np = Perceptron()\np.fit(X, y, max_steps = 1000)\nfig = plt.plot(p.history)\nxlab = plt.xlabel(\"Iteration\")\nylab = plt.ylabel(\"Accuracy\")\n\n\n\n\nSimilar to the results from experiment 2, this data set is also not linearly separable because the accuracy never reaches 1.00. It was never able to converge, hence the oscillation of the accuracy scores.\n\n\n\n\n\nThe runtime complexity of a single iteration of the perceptron algorithm update would depend on p, which is the number of features. Finding the \\(\\langle{\\tilde{w}^{(t)},\\tilde{x}_{i}}\\rangle\\) will probably take O(p) time. There are other math operations, however they probably aren’t significant in terms of overall time. Therefore, I think the runtime is O(p)."
  },
  {
    "objectID": "posts/logisticRegression/logisticRegression.html",
    "href": "posts/logisticRegression/logisticRegression.html",
    "title": "My Awesome CSCI 0451 Blog",
    "section": "",
    "text": "Source code: logisticRegression.py\n\n\n\nThis blog post will cover gradient descent and a key variant of the gradient descent called stochastic gradient descent for the logistic regression algorithm. This is an algorithm that uses a logistic function to model the relationship between the input features and the probability of a binary outcome (i.e., “yes” or “no” prediction).\n\n\n\n\nIn our LogisticRegression class, we implemented a fit method and a fit_stochastic method. Both methods take in some input data, X, the target labels, y, as well as hyperparameters alpha and max_epochs. The hyperparameter alpha is the learning rate for the gradient descent algorithm, and max_epochs is the maximum number of iterations the algorithm should run for. The fit_stochastic method also has two additional hyperparameters: batch_size and momentum. The hyperparameters batch_size, is the number of samples to use for each mini-batch during stochastic gradient descent, and momentum is a boolean for whether or not to use momentum in the optimization process.\nIn the fit method, a random weight vector self.w is initialized with shape (p,), where p is the number of features in the modified input data. The method enters a loop that iterates over max_epochs, and with each iteration, it computes the gradient of the loss function with respects to the weight vector by using the gradient method. This method implements the gradient of the empirical risk for logistic regression:\n\nWith each iteration, the method also performs a gradient step to update the weight vector, computes the loss and score on the training data using the updated weight vector, and stores the loss and score histories in self.loss_history and self.score_history. If the difference in loss between consecutive iterations is small, the loop will terminate early.\nThe fit_stochastic method is similar to the fit method, except it computes the gradient descent on batches of the data instead of the whole data. It updates the weight vector with the gradient and momentum, if enabled, accordingly. It then computes the loss and adds it to a list, and checks if the new loss is close enough to the previous loss to terminate early.\n\n\n\n\nBefore starting, you need to load all of these lilbraries and modules:\n\nfrom logisticRegression import LogisticRegression\nfrom sklearn.datasets import make_blobs\nfrom matplotlib import pyplot as plt\nimport numpy as np\nnp.seterr(all='ignore') \n\ndef graph_lossNline(X, y, LR):\n    '''\n    Visualizes the performance of the classifier and check the value of the loss it achieves\n    \n    Parameters:\n        X: Input features of the dataset\n        y: Labels of the dataset\n        LR: Instance of a class with the follow attributes:\n            - w: model parameters\n            - loss_history: empirical risk of the model after each iteration of gradient descent\n            \n    Returns:\n        None\n        \n    Plots:\n        Subplot 1: Scatter plot of the input features, colored according to their labels. The decision boundary of the model is plotted in black\n        Subplot 2: Line plot of the empirical risk of the model as a function of the iteration number\n        \n    Referenced: https://middlebury-csci-0451.github.io/CSCI-0451/lecture-notes/gradient-descent.html\n    '''\n    fig, axarr = plt.subplots(1, 2)\n\n    axarr[0].scatter(X[:,0], X[:,1], c = y)\n    axarr[0].set(xlabel = \"Feature 1\", ylabel = \"Feature 2\", title = f\"Loss = {LR.loss_history[-1]}\")\n\n    f1 = np.linspace(-3, 3, 101)\n\n    p = axarr[0].plot(f1, (- LR.w[2] - f1*LR.w[0])/LR.w[1], color = \"black\")\n\n    axarr[1].plot(LR.loss_history)\n    axarr[1].set(xlabel = \"Iteration number\", ylabel = \"Empirical Risk\")\n    plt.tight_layout()\n\nYou also want to understand how fit(gradient descent) and fit_stochastic (stochastic gradient descent) work. Here is an example for each method:\n\n# make the data\np_features = 3\nX, y = make_blobs(n_samples = 200, n_features = p_features - 1, centers = [(-1, -1), (1, 1)])\n\nfig = plt.scatter(X[:,0], X[:,1], c = y)\nxlab = plt.xlabel(\"Feature 1\")\nylab = plt.ylabel(\"Feature 2\")\n\n\n\n\nHere we generated a data set with two features. Notice how their labels are overlapping each other. We can then fit this data using fit and fit_stochastic with a reasonable learning rate so that they would converge.\n\n# fit the model\n\nLR = LogisticRegression()\nLR.fit(X, y, alpha = 0.1, max_epochs = 1000)\n\ngraph_lossNline(X, y, LR)\n\n\n\n\n\n# fit the model\n\nLR_stochastic = LogisticRegression()\nLR_stochastic.fit_stochastic(X, y, alpha = 0.1, max_epochs = 1000, batch_size = 10)\n\ngraph_lossNline(X, y, LR_stochastic)\n\n\n\n\n\nnum_steps = len(LR_stochastic.loss_history)\nplt.plot(np.arange(num_steps) + 1, LR_stochastic.loss_history, label = \"stochastic gradient\")\n\nnum_steps = len(LR.loss_history)\nplt.plot(np.arange(num_steps) + 1, LR.loss_history, label = \"gradient\")\n\nplt.loglog()\n\nlegend = plt.legend() \n\n\n\n\nIn the visualization above, you can see that the data was able to converge for both methods. The logistic regression with stochastic gradient descent seems to converge faster than logistic regression with regular gradient descent. This may be because stochastic gradient descent updates the weight based on the gradient of the loss function with respect to a small batch of data at a time, while the regular gradient descent is based on the entire training set.\n\n\n\nThis experiment tests the influence of a high learning rate on the convergence of gradient descent and stochastic descent to a minimizer.\n\nLR = LogisticRegression()\nLR.fit(X, y, alpha = 66, max_epochs = 1000)\n\ngraph_lossNline(X, y, LR)\n\n\n\n\n\n# fit the model\n\nLR_stochastic = LogisticRegression()\nLR_stochastic.fit_stochastic(X, y, alpha = 66, max_epochs = 1000, batch_size = 10)\n\ngraph_lossNline(X, y, LR_stochastic)\n\n\n\n\n\nnum_steps = len(LR_stochastic.loss_history)\nplt.plot(np.arange(num_steps) + 1, LR_stochastic.loss_history, label = \"stochastic gradient\")\n\nnum_steps = len(LR.loss_history)\nplt.plot(np.arange(num_steps) + 1, LR.loss_history, label = \"gradient\")\n\nplt.loglog()\n\nlegend = plt.legend() \n\n\n\n\nIt is important to keep the learning rate small to ensure that the algorithm converges stably and the optimization process is under control. If the learning rate is too large, the optimization algorithm can overshoot the minimum of the loss function and fail to converge to an optimal solution.\nYou can see that with increasing the learning rate to 66 from the 0.1 that it was previously set to, the loss function’s values (normal gradient descent) vary significantly instead of decreasing steadily and never converged. The one with stochastic gradient descent failed to converge so badly that it’s loss is not even a number.\n\n\n\n\nThis experiment tests the influence of batch size on how quickly the algorithm converges.\n\n# Make the data\np_features = 19\nX, y = make_blobs(n_samples = 888, n_features = p_features - 1, centers = [(-1, -1), (1, 1)])\n\ndef graph_loss(LR):\n    ''' \n    Plots the loss history of a logistic regression model\n    '''\n    fig, axarr = plt.subplots(1, 1)\n\n    axarr.plot(LR.loss_history)\n    axarr.set(xlabel = \"Iteration number\", ylabel = \"Empirical Risk\", title = f\"Loss = {LR.loss_history[-1]}\")\n    plt.tight_layout()\n\n\nLR_8000 = LogisticRegression()\nLR_8000.fit_stochastic(X, y, alpha = 0.1, max_epochs = 8888, batch_size = 800)\n\ngraph_loss(LR_8000)\n\n\n\n\n\nLR_800 = LogisticRegression()\nLR_800.fit_stochastic(X, y, alpha = 0.1, max_epochs = 8888, batch_size = 80)\n\ngraph_loss(LR_800)\n\n\n\n\n\nLR_80 = LogisticRegression()\nLR_80.fit_stochastic(X, y, alpha = 0.1, max_epochs = 8888, batch_size = 8)\n\ngraph_loss(LR_80)\n\n\n\n\nFor this experiment, we generated a data set that has 888 data points and 18 features. From there, we visualized the loss history of the algorithm if the batch size was 800, 80, and 8. With every decrease in batch size, there was a decrease in the number of iterations done. It is also important to note that when the batch size was 8, there was a bit more fluctuation in the loss than when the batch size was larger. Overall, we can see that a smaller batch size will result in faster convergence, although there will be a bit more variations in loss, and a larger batch size will result in a smoother convergence.\n\n\n\n\nMomentum is a technique used to accerlate the convergence of the optimization algorithm. It adds a fraction of the previous gradient to the current gradient during each iteration. The overall idea is that if the previous weight update was good, keep heading in that direction. The experiment below shows that momentum reduces the number of iterations done.\n\np_features = 3\nX, y = make_blobs(n_samples = 200, n_features = p_features - 1, centers = [(-1, -1), (1, 1)])\n\nfig = plt.scatter(X[:,0], X[:,1], c = y)\nxlab = plt.xlabel(\"Feature 1\")\nylab = plt.ylabel(\"Feature 2\")\n\n\n\n\n\nLR = LogisticRegression()\nLR.fit_stochastic(X, y, alpha = 0.1, max_epochs = 10000, batch_size = 10, momentum = False)\n\ngraph_lossNline(X, y, LR)\n\n\n\n\n\nLR.fit_stochastic(X, y, alpha = 0.1, max_epochs = 10000, batch_size = 10, momentum = True)\n\ngraph_lossNline(X, y, LR)"
  },
  {
    "objectID": "posts/dr-timnit-gebru/dr-timnit-gebru.html",
    "href": "posts/dr-timnit-gebru/dr-timnit-gebru.html",
    "title": "My Awesome CSCI 0451 Blog",
    "section": "",
    "text": "Dr. Gebru’s Talk (Video)\nIn her talk at the conference on Computer Vision and Pattern Recogition 2020 as part of a Tutorial on Fairness, Accountability, Transparency, and Ethics (FATE) in Computer Vision, Dr. Gebru provided an overview of the key issues and challenges in developing ethical and fair computer vision systems.\nShe highlighted the ways in chich bias and discrimination can be embedded in the data used to train these systems, as well as the algorithms and models themselves. For example, she noted that if a training data set contains imbalances in representation across different groups, then the resulting model may be diased towards those groupds that are overrepresented in the data. This can lead to unfair outcomes, such as misclassification or exclusion of certain individuals or groups.\nDr. Gebru suggested several approaches for ensuring fairness in computer vision systems, such as using representative data sets, measuring and mitigating bias in algorithms and models, and evaluating these systems on multiple fairness metrics, as different metrics may be more relevant to different applications or contexts. She emphasized the need for greater transparency and accountability in the development and deployment of computer vision systems, especially in high-stakes applications such as criminal justice and healthcare.\nA key takeaway from her talk is that computer vision, as used today, aren’t a neutral or objective technology. These systems can have significant impacts on individuals or society as a whole, particularly in areas such as criminal justice, healthcare, and employment.\n\n\n\nQuestion\nA question for Dr. Gebru is, what steps do you believe are necessary to ensure that the development and deployment of AI systems are guided by ethical principles, and how can these principles be integrated into the broader tech industry culture especially when those who’ve spoken out about these problems have been “punished” by big tech corporations?\n\n\nDr. Gebru’s Talk “At” Middlebury (Summary and Reflection)\nA big takeaway of Dr. Gebru’s talk is the whole concept of AGI (Artifical General Intelligence) and the parties that are involved, more specifically who’s involved in the development of AGI, what’s the purpose it’s trying to achieve, and who’s actually benefitting from it. She stated there are various definitions of AGI, one of which is “an autonomous system that surpasses human capabilities in the majority of economically valuable tasks (Wikipedia), however, there is no one definition that is agreed upon all across the board. Despite not being a well-defined term, it is generally advertised as being able to solve everything for everyone. This sounds wonderful at first (there’s a system that can solve all of our problems!!), however she continued on by pointing out critical problems underlying this concept of AGI.\nDr. Gebru highlighted the unethical issues that are not talked about in media, including the exploited labor behind artifical intelligence. She explained how Kenyan workers moderate ChatGBT for less than $2 per hour. Not only is monetary compensation an issue, but also the fact that many of the workers end up traumatized by the toxic questions and comments that they need to filter out. Another problem she pointed out is how resources are not going to organizations around the world to serve their own communities, rather it’s going to one big corporation. Dr. Gebru also noted that with this whole advertising of AGI being able to solve all problems, that could include health problems as well– pointing out how it could eventually lead to a situation where poor people would get a chatbox, while rich people would get an actual doctor.\nI don’t know how I feel after listening to her talk. I think for some parts of her argument, I couldn’t really grasp/follow along, and maybe that’s because of my limited background knowledge on what she was saying. However, it really shocked me when she shared things like the what Kenyan workers have to deal with or enhancing humans by merging their minds with machine (near the end of her talk)– which is just absurd to me. There was a lot to unravel from her talk, and it left me with questions like what should we do then? Since technology is advancing everyday and more and more attention are given to AI, how can we prevent the situation from becoming too extreme? Is that even possible considering the scale of everything."
  },
  {
    "objectID": "posts/linearRegression/linearRegression.html",
    "href": "posts/linearRegression/linearRegression.html",
    "title": "My Awesome CSCI 0451 Blog",
    "section": "",
    "text": "Source code linearRegression.py\n\n\n\nThis blog post will cover the implementation of least-squares linear regression. It will also go over results from experimenting with LASSO regularization for overparameterized problems.\n\n\n\n\nLinear regression is a model used to understand and quantify the relationship between variables and an outcome.\nThis equation represents the empirical risk minimization problem that we want to solve.\n\nTo solve this problem, we first take the gradient with respects to \\(\\hat{w}\\). Eventually we want to solve for \\(\\hat{w}\\). The equation below is the explicit formula for \\(\\hat{w}\\).\n\nIn our LinearRegression class, we implemented a fit_analytic method using this explicit formula for \\(\\hat{w}\\). We also implemented a fit_gradient method, which is gradient descent for linear regression. To implement fit_gradient, we used the formula\n\nWith each weight update, the gradient multiplied by the alpha value (default of 0.01) was subtracted from the weight to obtain a new weight.\n\n\n\n\nBefore starting, you need to load all of these libraries, modules, and functions.\n\nfrom linearRegression import LinearRegression\nimport numpy as np\nfrom matplotlib import pyplot as plt\nfrom sklearn.linear_model import Lasso\n\ndef pad(X):\n    return np.append(X, np.ones((X.shape[0], 1)), 1)\n\ndef LR_data(n_train = 100, n_val = 100, p_features = 1, noise = .1, w = None):\n    if w is None: \n        w = np.random.rand(p_features + 1) + .2\n    \n    X_train = np.random.rand(n_train, p_features)\n    y_train = pad(X_train)@w + noise*np.random.randn(n_train)\n\n    X_val = np.random.rand(n_val, p_features)\n    y_val = pad(X_val)@w + noise*np.random.randn(n_val)\n    \n    return X_train, y_train, X_val, y_val\n\n\n\n\n\nTo demonstrate that we’ve implemented LinearRegression correctly, we are going to test it on a data set that has 100 data points and 1 feature.\n\nn_train = 100\nn_val = 100\np_features = 1\nnoise = 0.2\n\n# create some data\nX_train, y_train, X_val, y_val = LR_data(n_train, n_val, p_features, noise)\n\n# plot it\nfig, axarr = plt.subplots(1, 2, sharex = True, sharey = True)\naxarr[0].scatter(X_train, y_train)\naxarr[1].scatter(X_val, y_val)\nlabs = axarr[0].set(title = \"Training\", xlabel = \"x\", ylabel = \"y\")\nlabs = axarr[1].set(title = \"Validation\", xlabel = \"x\")\nplt.tight_layout()\n\n\n\n\nHere we fit our model using the class we implemented and test out the fit_analytic method.\n\nLR = LinearRegression()\nLR.fit_analytic(X_train, y_train) # I used the analytical formula as my default fit method\n\nprint(f\"Training score = {LR.score(pad(X_train), y_train).round(4)}\")\nprint(f\"Validation score = {LR.score(pad(X_val), y_val).round(4)}\")\n\nTraining score = 0.6965\nValidation score = 0.7433\n\n\nWe can check the estimated weight vector w\n\nLR.w\n\narray([1.11225311, 1.22342203])\n\n\nWe can use the fit_gradient method we implemented to get the same value for w as well.\n\nLR2 = LinearRegression()\n\nLR2.fit_gradient(X_train, y_train, alpha = 0.01, max_iter = 1e2)\nLR2.w\n\narray([1.11236121, 1.22336123])\n\n\nHere we plot the change in score over time.\n\nplt.plot(LR2.score_history)\nlabels = plt.gca().set(xlabel = \"Iteration\", ylabel = \"Score\")\n\n\n\n\n\n\n\nBelow we’ll be looking at an experiment in which the number of features used, p_features, will increase while the number of training points, n_train, stays constant.\n\nn_train = 100\nn_val = 100\np_features = 1\nnoise = 0.2\n\ntraining_scores = []\nvalidation_scores = []\n\nwhile (p_features < n_train):\n    # create some data\n    X_train, y_train, X_val, y_val = LR_data(n_train, n_val, p_features, noise)\n    \n    LR = LinearRegression()\n    \n    LR.fit_analytic(X_train, y_train)\n    \n    train_scores = LR.score(pad(X_train), y_train)\n    val_scores = LR.score(pad(X_val), y_val)\n    \n    training_scores.append(train_scores)\n    validation_scores.append(val_scores)\n    \n    p_features += 1\n    \nplt.plot(training_scores, label = \"Training\")\nplt.plot(validation_scores, label = \"Validation\")\n\nplt.xscale('log')\nlegend = plt.legend() \nlabels = plt.gca().set(xlabel = \"# Features\", ylabel = \"Score\")\n\n\n\n\nYou can see in the graph above that as the number of features used increased, the training score also increased. The validation score, however, experienced a lot of oscilation as the number of features increased and eventually the score dropped by the end. This is an indication of overfitting since the difference between our training score and validation score increased a lot with the number of features.\n\n\n\nBelow are some experiments done using the LASSO algorithm from scikit-learn. This algorithm has a modified loss function with a regularization term, which makes the entries of the weight vector w small. It tends to force entries of the weight vector to be exactly zero.\n\nn_train = 100\nn_val = 100\np_features = 1\nnoise = 0.2\n\ntraining_scores = []\nvalidation_scores = []\n\nwhile (p_features < n_train):\n    # create some data\n    X_train, y_train, X_val, y_val = LR_data(n_train, n_val, p_features, noise)\n    \n    L = Lasso(alpha = 0.001)\n    \n    L.fit(X_train, y_train)\n    \n    train_scores = L.score(X_train, y_train)\n    val_scores = L.score(X_val, y_val)\n    \n    training_scores.append(train_scores)\n    validation_scores.append(val_scores)\n    \n    p_features += 1\n    \nplt.plot(training_scores, label = \"Training\")\nplt.plot(validation_scores, label = \"Validation\")\n\nplt.xscale('log')\nlegend = plt.legend() \nlabels = plt.gca().set(xlabel = \"# Features\", ylabel = \"Score\")\n\n\n\n\nIn this first experiment, we are working with 100 data points and 1 feature. The only difference between this and the previous experiment (Linear Regression) is the algorithm. With an alpha value of 0.001, the training accuracy increases with the number of features, but the validation accuracy decreases. We can see that the model was overfitting towards the end. These results are similar to the standard linear regression version.\n\nn_train = 100\nn_val = 100\np_features = 1\nnoise = 0.2\n\ntraining_scores = []\nvalidation_scores = []\n\nwhile (p_features < n_train):\n    # create some data\n    X_train, y_train, X_val, y_val = LR_data(n_train, n_val, p_features, noise)\n    \n    L = Lasso(alpha = 0.1)\n    \n    L.fit(X_train, y_train)\n    \n    train_scores = L.score(X_train, y_train)\n    val_scores = L.score(X_val, y_val)\n    \n    training_scores.append(train_scores)\n    validation_scores.append(val_scores)\n    \n    p_features += 1\n    \nplt.plot(training_scores, label = \"Training\")\nplt.plot(validation_scores, label = \"Validation\")\n\nplt.xscale('log')\nlegend = plt.legend() \nlabels = plt.gca().set(xlabel = \"# Features\", ylabel = \"Score\")\n\n\n\n\nIn this second experiment, we increased the alpha value to 0.1. With this increase, there is a lot more oscilation in both training and validation accuracy. Due to these oscilations, it seems that the training accuracy was not able to reach a perfect score of 1.0, like we observed in the linear regression version. The validation doesn’t really dip either, though that’s only because it never steadily increased in the first place.\n\n\nn_train = 100\nn_val = 100\np_features = 1\nnoise = 0.2\n\ntraining_scores = []\nvalidation_scores = []\n\nwhile (p_features < n_train + 999):\n    # create some data\n    X_train, y_train, X_val, y_val = LR_data(n_train, n_val, p_features, noise)\n    \n    L = Lasso(alpha = 0.1)\n    \n    L.fit(X_train, y_train)\n    \n    train_scores = L.score(X_train, y_train)\n    val_scores = L.score(X_val, y_val)\n    \n    training_scores.append(train_scores)\n    validation_scores.append(val_scores)\n    \n    p_features += 1\n    \nplt.plot(training_scores, label = \"Training\")\nplt.plot(validation_scores, label = \"Validation\")\n\nplt.xscale('log')\nlegend = plt.legend() \nlabels = plt.gca().set(xlabel = \"# Features\", ylabel = \"Score\")\n\n\n\n\nIn this final experiment, the alpha value is set to 0.1 and the number of feature is going post the number of data points we have. You can see the oscilation that was present in the previous experiment, but there’s more of a distinction in training scores and validation scores that’s reminiscent of the linear regression version."
  },
  {
    "objectID": "posts/auditing-bias/Untitled.html",
    "href": "posts/auditing-bias/Untitled.html",
    "title": "My Awesome CSCI 0451 Blog",
    "section": "",
    "text": "possible_features=['AGEP', 'SCHL', 'MAR', 'RELP', 'DIS', 'ESP', 'CIT', 'MIG', 'MIL', 'ANC', 'NATIVITY', 'DEAR', 'DEYE', 'DREM', 'SEX', 'RAC1P', 'ESR']\nacs_data[possible_features].head()\n\n\n\n\n\n  \n    \n      \n      AGEP\n      SCHL\n      MAR\n      RELP\n      DIS\n      ESP\n      CIT\n      MIG\n      MIL\n      ANC\n      NATIVITY\n      DEAR\n      DEYE\n      DREM\n      SEX\n      RAC1P\n      ESR\n    \n  \n  \n    \n      0\n      19\n      18.0\n      5\n      17\n      2\n      NaN\n      1\n      3.0\n      4.0\n      1\n      1\n      2\n      2\n      2.0\n      2\n      1\n      6.0\n    \n    \n      1\n      18\n      18.0\n      5\n      17\n      2\n      NaN\n      1\n      3.0\n      4.0\n      1\n      1\n      2\n      2\n      2.0\n      2\n      2\n      6.0\n    \n    \n      2\n      53\n      17.0\n      5\n      16\n      1\n      NaN\n      1\n      1.0\n      4.0\n      2\n      1\n      2\n      2\n      1.0\n      1\n      1\n      6.0\n    \n    \n      3\n      28\n      19.0\n      5\n      16\n      2\n      NaN\n      1\n      1.0\n      2.0\n      1\n      1\n      2\n      2\n      2.0\n      1\n      1\n      6.0\n    \n    \n      4\n      25\n      12.0\n      5\n      16\n      1\n      NaN\n      1\n      3.0\n      4.0\n      1\n      1\n      2\n      2\n      1.0\n      2\n      1\n      6.0\n    \n  \n\n\n\n\n\nfeatures_to_use = [f for f in possible_features if f not in [\"ESR\", \"RAC1P\"]]\n\n\nEmploymentProblem = BasicProblem(\n    features=features_to_use,\n    target='ESR',\n    target_transform=lambda x: x == 1,\n    group='RAC1P',\n    preprocess=lambda x: x,\n    postprocess=lambda x: np.nan_to_num(x, -1),\n)\n\nfeatures, label, group = EmploymentProblem.df_to_numpy(acs_data)\n\n\nfor obj in [features, label, group]:\n  print(obj.shape)\n\n(47777, 15)\n(47777,)\n(47777,)\n\n\n\nfrom sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test, group_train, group_test = train_test_split(\n    features, label, group, test_size=0.2, random_state=0)\n\n\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.pipeline import make_pipeline\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.metrics import confusion_matrix\n\nmodel = make_pipeline(StandardScaler(), LogisticRegression())\nmodel.fit(X_train, y_train)\n\nPipeline(steps=[('standardscaler', StandardScaler()),\n                ('logisticregression', LogisticRegression())])In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.PipelinePipeline(steps=[('standardscaler', StandardScaler()),\n                ('logisticregression', LogisticRegression())])StandardScalerStandardScaler()LogisticRegressionLogisticRegression()\n\n\n\ny_hat = model.predict(X_test)\n\n\n(y_hat == y_test).mean()\n\n0.7842193386354123\n\n\n\n(y_hat == y_test)[group_test == 1].mean()\n\n0.7838255977496483\n\n\n\n(y_hat == y_test)[group_test == 2].mean()\n\n0.7838630806845965"
  },
  {
    "objectID": "posts/palmer-penguins/palmer_penguins.html",
    "href": "posts/palmer-penguins/palmer_penguins.html",
    "title": "My Awesome CSCI 0451 Blog",
    "section": "",
    "text": "In this blog post, we’ll walk through a complete example of the standard machine learning workflow using the Palmer Penguins data set. The main objective is to determine a set of features that can be used to correctly determine the species of a penguin.\n\n\n\n\nBefore starting, you need to load all of these libraries and modules:\n\nimport pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport numpy as np\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.tree import DecisionTreeClassifier \nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.svm import SVC \nfrom itertools import combinations\nfrom sklearn.model_selection import cross_val_score\nfrom matplotlib.patches import Patch\n\n\n\n\n\nHere we are loading in the data and preparing the qualitive columns in the data set.\n\ntrain_url = \"https://raw.githubusercontent.com/middlebury-csci-0451/CSCI-0451/main/data/palmer-penguins/train.csv\"\ntrain = pd.read_csv(train_url)\n\n#train.head()\n\nle = LabelEncoder()\nle.fit(train[\"Species\"])\n\ndef prepare_data(df):\n  df = df.drop([\"studyName\", \"Sample Number\", \"Individual ID\", \"Date Egg\", \"Comments\", \"Region\"], axis = 1)\n  # Remove any rows where the value in the \"Sex\" column is \".\"\n  df = df[df[\"Sex\"] != \".\"]\n  df = df.dropna()\n  y = le.transform(df[\"Species\"])\n  df = df.drop([\"Species\"], axis = 1)\n  df = pd.get_dummies(df)\n  return df, y\n\nX_train, y_train = prepare_data(train)\n\n#X_train\n\n\n\n\nTo learn a little more about the data set, we grouped the data by sex and body mass (g). A 1 in the Sex_MALE column represents the male penguins, and the 0 represents the female penguins. As you can see, the body mass differs greatly between the two sex (~800 g). While this is information is not needed for species classification (what we are trying to do in this blog post), this is interesting information for a penguin sex classification.\n\ndata = X_train.groupby('Sex_MALE')['Body Mass (g)'].mean()\ndf = pd.DataFrame(data)\ndf\n\n\n\n\n\n  \n    \n      \n      Body Mass (g)\n    \n    \n      Sex_MALE\n      \n    \n  \n  \n    \n      0\n      3823.214286\n    \n    \n      1\n      4613.076923\n    \n  \n\n\n\n\nWe also looked into the possible relationship between culmen length (mm) and body mass (g) on each island (Island Dream, Island Biscoe, and Island Torgersen). While these aren’t the best graphs, you can see that there’s a big difference between the culmen length (mm) and body mass (g) on each island. The graph for Island Biscoe shows a very linear relationship between culmen length (mm) and body mass (g). Could this be a sign that Island Biscoe has a more homogeneous penguin (species) population? It’s also interesting to note that the penguins on Island Torgersen seem to have smaller culmen length (mm) and body mass (g) than the other two islands. However, it also seems to be more varied– not as clear of a relationship. Does this mean that Island Torgersen has a more varied penguin (species) population?\n\n# Generate the three plots\ng1 = sns.lmplot(\n    data = X_train,\n    x = \"Culmen Length (mm)\",\n    y = \"Body Mass (g)\",\n    hue = \"Island_Dream\"\n)\ng2 = sns.lmplot(\n    data = X_train,\n    x = \"Culmen Length (mm)\",\n    y = \"Body Mass (g)\",\n    hue = \"Island_Biscoe\"\n)\ng3 = sns.lmplot(\n    data = X_train,\n    x = \"Culmen Length (mm)\",\n    y = \"Body Mass (g)\",\n    hue = \"Island_Torgersen\"\n)\n\n# Add titles to the subplots\ng1.ax.set_title(\"Island_Dream\")\ng2.ax.set_title(\"Island_Biscoe\")\ng3.ax.set_title(\"Island_Torgersen\")\n\nText(0.5, 1.0, 'Island_Torgersen')\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nHere we are choosing the features (at least one qualitative and at least two quantitative). To do so, we are using the exhaustive search approach to go through all the features contained in the data set. We define two lists that contains the names of the qualitative and quantitative data in the data set, and we create a pandas dataframe that would later store the results of the cross-validation evaluation for each combination of features. We iterate through the two lists we created. We create a new LogisticRegression object and we use cross-validation to evaluate the performance of the logistic regression model using the current combination of features.\n\n#selecting the columns that we want\nall_qual_cols = [\"Clutch Completion\", \"Sex\", \"Island\", \"Stage\"]\nall_quant_cols = ['Culmen Length (mm)', 'Culmen Depth (mm)', 'Flipper Length (mm)', 'Body Mass (g)']\n\nscores_df = pd.DataFrame(columns=['Columns', 'Score'])\n\nfor qual in all_qual_cols: \n    qual_cols = [col for col in X_train.columns if qual in col]\n  \n    for pair in combinations(all_quant_cols, 2):\n        cols = qual_cols + list(pair)  \n\n        lr = LogisticRegression(max_iter = 10000) # included the max number of iterations because I kept getting warnings\n        \n        cv_mean = cross_val_score(lr, X_train[cols], y_train, cv = 10).mean()\n        scores_df = pd.concat([scores_df, pd.DataFrame({'Columns': cols, 'Score': cv_mean})], ignore_index=True)\n\nscores_df = scores_df.sort_values(by='Score', ascending=False).reset_index(drop=True) # sort score in decreasing order\n\nscores_df.head()\nfeatures = ['Culmen Length (mm)', 'Culmen Depth (mm)', 'Island_Biscoe', 'Island_Dream', 'Island_Torgersen']\n\nAfter choosing our features, we train our model with those features. We get a 0.996 or 99.6% training accuracy. We then test the model and get a 1.0 or 100% testing accuracy.\n\nlr = LogisticRegression(max_iter = 1000)\nlr.fit(X_train[features], y_train)\nlr.score(X_train[features], y_train)\n\n0.99609375\n\n\n\ntest_url = \"https://raw.githubusercontent.com/middlebury-csci-0451/CSCI-0451/main/data/palmer-penguins/test.csv\"\ntest = pd.read_csv(test_url)\n\nX_test, y_test = prepare_data(test)\n\nlr.score(X_test[features], y_test)\n\n1.0\n\n\nWe also tried training on a RandomForestClassifier model.\n\nrf = RandomForestClassifier()\nrf.fit(X_train[features], y_train)\nrf.score(X_train[features], y_train)\n\n1.0\n\n\n\nrf.score(X_test[features], y_test)\n\n0.9852941176470589\n\n\n\n\n\nUsing the follow function, we will show the decision regions of the finished models.\n\ndef plot_regions(model, X, y):\n    \n    x0 = X[X.columns[0]]\n    x1 = X[X.columns[1]]\n    qual_features = X.columns[2:]\n    \n    fig, axarr = plt.subplots(1, len(qual_features), figsize = (7, 3))\n\n    # create a grid\n    grid_x = np.linspace(x0.min(),x0.max(),501)\n    grid_y = np.linspace(x1.min(),x1.max(),501)\n    xx, yy = np.meshgrid(grid_x, grid_y)\n    \n    XX = xx.ravel()\n    YY = yy.ravel()\n\n    for i in range(len(qual_features)):\n        XY = pd.DataFrame({\n          X.columns[0] : XX,\n          X.columns[1] : YY\n      })\n\n        for j in qual_features:\n            XY[j] = 0\n            \n        XY[qual_features[i]] = 1\n            \n        p= model.predict(XY)\n        p = p.reshape(xx.shape)\n      \n      \n        # use contour plot to visualize the predictions\n        axarr[i].contourf(xx, yy, p, cmap = \"jet\", alpha = 0.2, vmin = 0, vmax = 2)\n      \n        ix = X[qual_features[i]] == 1\n        # plot the data\n        axarr[i].scatter(x0[ix], x1[ix], c = y[ix], cmap = \"jet\", vmin = 0, vmax = 2)\n      \n        axarr[i].set(xlabel = X.columns[0], \n            ylabel  = X.columns[1])\n      \n        patches = []\n        for color, spec in zip([\"red\", \"green\", \"blue\"], [\"Adelie\", \"Chinstrap\", \"Gentoo\"]):\n            patches.append(Patch(color = color, label = spec))\n\n        plt.legend(title = \"Species\", handles = patches, loc = \"best\")\n      \n        plt.tight_layout()\n\nThese are the decision regions for the LogisticRegression model. As you can see, the model did a really good job at classifying the three penguin species.\n\nplot_regions(lr, X_train[features], y_train)\n\n\n\n\nThese are the decision regions for the RandomForestClassifier model. There may be a little bit of overfitting because of the jagged boundaries, however the model did a pretty good job at classifying the species.\n\nplot_regions(rf, X_train[features], y_train)"
  }
]