[
  {
    "objectID": "posts/example-blog-post/index.html",
    "href": "posts/example-blog-post/index.html",
    "title": "Hello Blog",
    "section": "",
    "text": "This is an example of the blog posts that you’ll submit as your primary form of learning demonstration in CSCI 0451. I created this post by modifying the file posts/example-blog-post/index.ipynb in VSCode. You can also use JupyterLab for this editing if you prefer. Finally, it is possible to write blog posts without using notebooks by writing .qmd files, as illustrated here."
  },
  {
    "objectID": "posts/example-blog-post/index.html#math",
    "href": "posts/example-blog-post/index.html#math",
    "title": "Hello Blog",
    "section": "Math",
    "text": "Math\nIn addition to regular text using the Markdown specification, you can also write mathematics, enclosed between dollar signs. The syntax for writing math is very similar to the syntax used in the \\(\\LaTeX\\) markup language. For example, $f(x) \\approx y$ renders to \\(f(x) \\approx y\\). To place complex mathematical expressions on their own lines, use double dollar signs. For example, the expression\n$$\\mathcal{L}(a, b) = \\sum_{i = 1}^n (ax_i + b - y_i)^2$$\nrenders to:\n\\[\\mathcal{L}(a, b) = \\sum_{i = 1}^n (ax_i + b - y_i)^2\\;.\\]\nBehind the scenes, math is powered by the MathJax engine. For more on how to write math, check this handy tutorial and quick reference."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "My Awesome CSCI 0451 Blog",
    "section": "",
    "text": "An example blog post illustrating the key techniques you’ll need to demonstrate your learning in CSCI 0451.\n\n\n\n\n\n\nJan 10, 2023\n\n\nPhil Chodrow\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "About this blog"
  },
  {
    "objectID": "posts/perceptron/perceptron.html",
    "href": "posts/perceptron/perceptron.html",
    "title": "My Awesome CSCI 0451 Blog",
    "section": "",
    "text": "Source code: perceptron.py\n\n\n\nThis blog post will cover the perceptron algorithm. This is a binary classification algorithm that takes in an input and finds a hyperplane that splits that input data such that all similar data points lie on either side of the hyperplane. This algorithm only works if the data is linearly separable.\n\n\n\n\nIn our Perceptron class, we implemented a fit(X, y) method that takes in an array of some data, X, and an array of labels, y, corresponding to the elements in the input data. Each element in y is either a 1 or a 0.\nIn the method, we started with a random value for weight, self.w, such that w = (w, -b). We also initalized a list, self.history, that would keep track of the scores over the training period. While the classification is not perfect (i.e., achieved a 100% accuracy rate) and the max number of iterations has not been reached, we will continue to update our w depending on if it was able to correctly classify a random data point, i, in our data set X. If it wasn’t able to correctly classify the random data point, we update w using the following equation:\n\nThis update is done in line 52 of our perceptron.py code. This update moves our separator in the right direction so that our randomly selected point could be correctly classified on the next iteration if it were to be selected again. As this loops continues, the weight is added to self.history.\n\n\n\n\nBefore starting, you need to load all of these libraries and modules:\n\nimport numpy as np\nimport pandas as pd\nimport seaborn as sns\nfrom matplotlib import pyplot as plt\nfrom perceptron import Perceptron\nfrom sklearn.datasets import make_blobs\n\n\n\n\nThis block of code generates a visualization that contains data that is linearly separable with 2 features.\n\nnp.random.seed(12345)\n\nn = 100\np_features = 3\n\nX, y = make_blobs(n_samples = 100, n_features = p_features - 1, centers = [(-1.7, -1.7), (1.7, 1.7)])\n\nfig = plt.scatter(X[:,0], X[:,1], c = y)\nxlab = plt.xlabel(\"Feature 1\")\nylab = plt.ylabel(\"Feature 2\")\n\n\n\n\nJust by looking at the graph, we can see that it is possible to have a line that separates all the purple “0” labels from the yellow “1” labels. To analyze it further, however, we can use the algorithm we implemented!\n\np = Perceptron()\np.fit(X, y, max_steps = 1000)\nfig = plt.plot(p.history)\nxlab = plt.xlabel(\"Iteration\")\nylab = plt.ylabel(\"Accuracy\")\n\n\n\n\nFrom the graph above, we can see that while there are variations in the accuracy, the algorithm does eventually achieved a perfect classification rate of 1.00. This means that the data was indeed separable and we were able to separate the data points into their respective labels.\n\ndef draw_line(w, x_min, x_max):\n  x = np.linspace(x_min, x_max, 101)\n  y = -(w[0]*x + w[2])/w[1]\n  plt.plot(x, y, color = \"black\")\n\nfig = plt.scatter(X[:,0], X[:,1], c = y)\nfig = draw_line(p.w, -2, 2)\n\nxlab = plt.xlabel(\"Feature 1\")\nylab = plt.ylabel(\"Feature 2\")\n\n\n\n\nHere is a visualization of the graph with a line separating the data points.\n\n\n\n\nThis block of code generates a visualization that contains data that is not linearly separable with 2 features.\n\nnp.random.seed(12345)\n\nn = 100\np_features = 3\n\nX, y = make_blobs(n_samples = 100, n_features = p_features - 1, centers = [(-1, -1), (1, 1)])\n\nfig = plt.scatter(X[:,0], X[:,1], c = y)\nxlab = plt.xlabel(\"Feature 1\")\nylab = plt.ylabel(\"Feature 2\")\n\n\n\n\n\np = Perceptron()\np.fit(X, y, max_steps = 1000)\nfig = plt.plot(p.history)\nxlab = plt.xlabel(\"Iteration\")\nylab = plt.ylabel(\"Accuracy\")\n\n\n\n\nHere we see the oscillation of accuracy. It never reaches 1.0 and the iterating stops once the max number of iterations is reached. This means the data is not linearly separable and the algorithm was not able to draw a line that would be able to separate the data points.\n\nfig = plt.scatter(X[:,0], X[:,1], c = y)\nfig = draw_line(p.w, -2, 2)\n\nxlab = plt.xlabel(\"Feature 1\")\nylab = plt.ylabel(\"Feature 2\")\n\n\n\n\nThis visualization shows that the algorithm was not able to find a line that would separate the data points accordingly.\n\n\n\n\nBelow is a block of code that generations a visualization of the evolution of the score over the training period of a random data set that contains 7 features.\n\nnp.random.seed(12345)\n\nn = 100\np_features = 8\n\nX, y = make_blobs(n_samples = 100, n_features = p_features - 1, centers = [(0, 0), (0, 0)])\n\np = Perceptron()\np.fit(X, y, max_steps = 1000)\nfig = plt.plot(p.history)\nxlab = plt.xlabel(\"Iteration\")\nylab = plt.ylabel(\"Accuracy\")\n\n\n\n\nSimilar to the results from experiment 2, this data set is also not linearly separable because the accuracy never reaches 1.00. It was never able to converge, hence the oscillation of the accuracy scores.\n\n\n\n\n\nThe runtime complexity of a single iteration of the perceptron algorithm update would depend on p, which is the number of features. Finding the \\(\\langle{\\tilde{w}^{(t)},\\tilde{x}_{i}}\\rangle\\) will probably take O(p) time. There are other math operations, however they probably aren’t significant in terms of overall time. Therefore, I think the runtime is O(p)."
  },
  {
    "objectID": "posts/logisticRegression/logisticRegression.html",
    "href": "posts/logisticRegression/logisticRegression.html",
    "title": "My Awesome CSCI 0451 Blog",
    "section": "",
    "text": "Source code: logisticRegression.py\n\n\n\nThis blog post will cover gradient descent and a key variant of the gradient descent called stochastic gradient descent for the logistic regression algorithm. This is an algorithm that uses a logistic function to model the relationship between the input features and the probability of a binary outcome (i.e., “yes” or “no” prediction).\n\n\n\n\nIn our LogisticRegression class, we implemented a fit method and a fit_stochastic method. Both methods take in some input data, X, the target labels, y, as well as hyperparameters alpha and max_epochs. The hyperparameter alpha is the learning rate for the gradient descent algorithm, and max_epochs is the maximum number of iterations the algorithm should run for. The fit_stochastic method also has two additional hyperparameters: batch_size and momentum. The hyperparameters batch_size, is the number of samples to use for each mini-batch during stochastic gradient descent, and momentum is a boolean for whether or not to use momentum in the optimization process.\nIn the fit method, a random weight vector self.w is initialized with shape (p,), where p is the number of features in the modified input data. The method enters a loop that iterates over max_epochs, and with each iteration, it computes the gradient of the loss function with respects to the weight vector by using the gradient method. This method implements the gradient of the empirical risk for logistic regression:\n\nWith each iteration, the method also performs a gradient step to update the weight vector, computes the loss and score on the training data using the updated weight vector, and stores the loss and score histories in self.loss_history and self.score_history. If the difference in loss between consecutive iterations is small, the loop will terminate early.\nThe fit_stochastic method is similar to the fit method, except it computes the gradient descent on batches of the data instead of the whole data. It updates the weight vector with the gradient and momentum, if enabled, accordingly. It then computes the loss and adds it to a list, and checks if the new loss is close enough to the previous loss to terminate early.\n\n\n\n\nBefore starting, you need to load all of these lilbraries and modules:\n\nfrom logisticRegression import LogisticRegression\nfrom sklearn.datasets import make_blobs\nfrom matplotlib import pyplot as plt\nimport numpy as np\nnp.seterr(all='ignore') \n\ndef graph_lossNline(X, y, LR):\n    '''\n    Visualizes the performance of the classifier and check the value of the loss it achieves\n    \n    Parameters:\n        X: Input features of the dataset\n        y: Labels of the dataset\n        LR: Instance of a class with the follow attributes:\n            - w: model parameters\n            - loss_history: empirical risk of the model after each iteration of gradient descent\n            \n    Returns:\n        None\n        \n    Plots:\n        Subplot 1: Scatter plot of the input features, colored according to their labels. The decision boundary of the model is plotted in black\n        Subplot 2: Line plot of the empirical risk of the model as a function of the iteration number\n        \n    Referenced: https://middlebury-csci-0451.github.io/CSCI-0451/lecture-notes/gradient-descent.html\n    '''\n    fig, axarr = plt.subplots(1, 2)\n\n    axarr[0].scatter(X[:,0], X[:,1], c = y)\n    axarr[0].set(xlabel = \"Feature 1\", ylabel = \"Feature 2\", title = f\"Loss = {LR.loss_history[-1]}\")\n\n    f1 = np.linspace(-3, 3, 101)\n\n    p = axarr[0].plot(f1, (- LR.w[2] - f1*LR.w[0])/LR.w[1], color = \"black\")\n\n    axarr[1].plot(LR.loss_history)\n    axarr[1].set(xlabel = \"Iteration number\", ylabel = \"Empirical Risk\")\n    plt.tight_layout()\n\nYou also want to understand how fit(gradient descent) and fit_stochastic (stochastic gradient descent) work. Here is an example for each method:\n\n# make the data\np_features = 3\nX, y = make_blobs(n_samples = 200, n_features = p_features - 1, centers = [(-1, -1), (1, 1)])\n\nfig = plt.scatter(X[:,0], X[:,1], c = y)\nxlab = plt.xlabel(\"Feature 1\")\nylab = plt.ylabel(\"Feature 2\")\n\n\n\n\nHere we generated a data set with two features. Notice how their labels are overlapping each other. We can then fit this data using fit and fit_stochastic with a reasonable learning rate so that they would converge.\n\n# fit the model\n\nLR = LogisticRegression()\nLR.fit(X, y, alpha = 0.1, max_epochs = 1000)\n\ngraph_lossNline(X, y, LR)\n\n\n\n\n\n# fit the model\n\nLR_stochastic = LogisticRegression()\nLR_stochastic.fit_stochastic(X, y, alpha = 0.1, max_epochs = 1000, batch_size = 10)\n\ngraph_lossNline(X, y, LR_stochastic)\n\n\n\n\n\nnum_steps = len(LR_stochastic.loss_history)\nplt.plot(np.arange(num_steps) + 1, LR_stochastic.loss_history, label = \"stochastic gradient\")\n\nnum_steps = len(LR.loss_history)\nplt.plot(np.arange(num_steps) + 1, LR.loss_history, label = \"gradient\")\n\nplt.loglog()\n\nlegend = plt.legend() \n\n\n\n\nIn the visualization above, you can see that the data was able to converge for both methods. The logistic regression with stochastic gradient descent seems to converge faster than logistic regression with regular gradient descent. This may be because stochastic gradient descent updates the weight based on the gradient of the loss function with respect to a small batch of data at a time, while the regular gradient descent is based on the entire training set.\n\n\n\nThis experiment tests the influence of a high learning rate on the convergence of gradient descent and stochastic descent to a minimizer.\n\nLR = LogisticRegression()\nLR.fit(X, y, alpha = 66, max_epochs = 1000)\n\ngraph_lossNline(X, y, LR)\n\n\n\n\n\n# fit the model\n\nLR_stochastic = LogisticRegression()\nLR_stochastic.fit_stochastic(X, y, alpha = 66, max_epochs = 1000, batch_size = 10)\n\ngraph_lossNline(X, y, LR_stochastic)\n\n\n\n\n\nnum_steps = len(LR_stochastic.loss_history)\nplt.plot(np.arange(num_steps) + 1, LR_stochastic.loss_history, label = \"stochastic gradient\")\n\nnum_steps = len(LR.loss_history)\nplt.plot(np.arange(num_steps) + 1, LR.loss_history, label = \"gradient\")\n\nplt.loglog()\n\nlegend = plt.legend() \n\n\n\n\nIt is important to keep the learning rate small to ensure that the algorithm converges stably and the optimization process is under control. If the learning rate is too large, the optimization algorithm can overshoot the minimum of the loss function and fail to converge to an optimal solution.\nYou can see that with increasing the learning rate to 66 from the 0.1 that it was previously set to, the loss function’s values (normal gradient descent) vary significantly instead of decreasing steadily and never converged. The one with stochastic gradient descent failed to converge so badly that it’s loss is not even a number.\n\n\n\n\nThis experiment tests the influence of batch size on how quickly the algorithm converges.\n\n# Make the data\np_features = 19\nX, y = make_blobs(n_samples = 888, n_features = p_features - 1, centers = [(-1, -1), (1, 1)])\n\ndef graph_loss(LR):\n    ''' \n    Plots the loss history of a logistic regression model\n    '''\n    fig, axarr = plt.subplots(1, 1)\n\n    axarr.plot(LR.loss_history)\n    axarr.set(xlabel = \"Iteration number\", ylabel = \"Empirical Risk\", title = f\"Loss = {LR.loss_history[-1]}\")\n    plt.tight_layout()\n\n\nLR_8000 = LogisticRegression()\nLR_8000.fit_stochastic(X, y, alpha = 0.1, max_epochs = 8888, batch_size = 800)\n\ngraph_loss(LR_8000)\n\n\n\n\n\nLR_800 = LogisticRegression()\nLR_800.fit_stochastic(X, y, alpha = 0.1, max_epochs = 8888, batch_size = 80)\n\ngraph_loss(LR_800)\n\n\n\n\n\nLR_80 = LogisticRegression()\nLR_80.fit_stochastic(X, y, alpha = 0.1, max_epochs = 8888, batch_size = 8)\n\ngraph_loss(LR_80)\n\n\n\n\nFor this experiment, we generated a data set that has 888 data points and 18 features. From there, we visualized the loss history of the algorithm if the batch size was 800, 80, and 8. With every decrease in batch size, there was a decrease in the number of iterations done. It is also important to note that when the batch size was 8, there was a bit more fluctuation in the loss than when the batch size was larger. Overall, we can see that a smaller batch size will result in faster convergence, although there will be a bit more variations in loss, and a larger batch size will result in a smoother convergence.\n\n\n\n\nMomentum is a technique used to accerlate the convergence of the optimization algorithm. It adds a fraction of the previous gradient to the current gradient during each iteration. The overall idea is that if the previous weight update was good, keep heading in that direction. The experiment below shows that momentum reduces the number of iterations done.\n\np_features = 3\nX, y = make_blobs(n_samples = 200, n_features = p_features - 1, centers = [(-1, -1), (1, 1)])\n\nfig = plt.scatter(X[:,0], X[:,1], c = y)\nxlab = plt.xlabel(\"Feature 1\")\nylab = plt.ylabel(\"Feature 2\")\n\n\n\n\n\nLR = LogisticRegression()\nLR.fit_stochastic(X, y, alpha = 0.1, max_epochs = 10000, batch_size = 10, momentum = False)\n\ngraph_lossNline(X, y, LR)\n\n\n\n\n\nLR.fit_stochastic(X, y, alpha = 0.1, max_epochs = 10000, batch_size = 10, momentum = True)\n\ngraph_lossNline(X, y, LR)"
  },
  {
    "objectID": "posts/dr-timnit-gebru/dr-timnit-gebru.html",
    "href": "posts/dr-timnit-gebru/dr-timnit-gebru.html",
    "title": "My Awesome CSCI 0451 Blog",
    "section": "",
    "text": "Dr. Gebru’s Talk (Video)\nIn her talk at the conference on Computer Vision and Pattern Recogition 2020 as part of a Tutorial on Fairness, Accountability, Transparency, and Ethics (FATE) in Computer Vision, Dr. Gebru provided an overview of the key issues and challenges in developing ethical and fair computer vision systems.\nShe highlighted the ways in chich bias and discrimination can be embedded in the data used to train these systems, as well as the algorithms and models themselves. For example, she noted that if a training data set contains imbalances in representation across different groups, then the resulting model may be diased towards those groupds that are overrepresented in the data. This can lead to unfair outcomes, such as misclassification or exclusion of certain individuals or groups.\nDr. Gebru suggested several approaches for ensuring fairness in computer vision systems, such as using representative data sets, measuring and mitigating bias in algorithms and models, and evaluating these systems on multiple fairness metrics, as different metrics may be more relevant to different applications or contexts. She emphasized the need for greater transparency and accountability in the development and deployment of computer vision systems, especially in high-stakes applications such as criminal justice and healthcare.\nA key takeaway from her talk is that computer vision, as used today, aren’t a neutral or objective technology. These systems can have significant impacts on individuals or society as a whole, particularly in areas such as criminal justice, healthcare, and employment.\n\n\n\nQuestion\nA question for Dr. Gebru is, what steps do you believe are necessary to ensure that the development and deployment of AI systems are guided by ethical principles, and how can these principles be integrated into the broader tech industry culture especially when those who’ve spoken out about these problems have been “punished” by big tech corporations?\n\n\nDr. Gebru’s Talk “At” Middlebury (Summary and Reflection)\nA big takeaway of Dr. Gebru’s talk is the whole concept of AGI (Artifical General Intelligence) and the parties that are involved, more specifically who’s involved in the development of AGI, what’s the purpose it’s trying to achieve, and who’s actually benefitting from it. She stated there are various definitions of AGI, one of which is “an autonomous system that surpasses human capabilities in the majority of economically valuable tasks (Wikipedia), however, there is no one definition that is agreed upon all across the board. Despite not being a well-defined term, it is generally advertised as being able to solve everything for everyone. This sounds wonderful at first (there’s a system that can solve all of our problems!!), however she continued on by pointing out critical problems underlying this concept of AGI.\nDr. Gebru highlighted the unethical issues that are not talked about in media, including the exploited labor behind artifical intelligence. She explained how Kenyan workers moderate ChatGBT for less than $2 per hour. Not only is monetary compensation an issue, but also the fact that many of the workers end up traumatized by the toxic questions and comments that they need to filter out. Another problem she pointed out is how resources are not going to organizations around the world to serve their own communities, rather it’s going to one big corporation. Dr. Gebru also noted that with this whole advertising of AGI being able to solve all problems, that could include health problems as well– pointing out how it could eventually lead to a situation where poor people would get a chatbox, while rich people would get an actual doctor.\nI don’t know how I feel after listening to her talk. I think for some parts of her argument, I couldn’t really grasp/follow along, and maybe that’s because of my limited background knowledge on what she was saying. However, it really shocked me when she shared things like the what Kenyan workers have to deal with or enhancing humans by merging their minds with machine (near the end of her talk)– which is just absurd to me. There was a lot to unravel from her talk, and it left me with questions like what should we do then? Since technology is advancing everyday and more and more attention are given to AI, how can we prevent the situation from becoming too extreme? Is that even possible considering the scale of everything."
  },
  {
    "objectID": "posts/linearRegression/linearRegression.html",
    "href": "posts/linearRegression/linearRegression.html",
    "title": "My Awesome CSCI 0451 Blog",
    "section": "",
    "text": "Source code linearRegression.py\n\n\n\nThis blog post will cover the implementation of least-squares linear regression. It will also go over results from experimenting with LASSO regularization for overparameterized problems.\n\n\n\n\nLinear regression is a model used to understand and quantify the relationship between variables and an outcome.\nThis equation represents the empirical risk minimization problem that we want to solve.\n\nTo solve this problem, we first take the gradient with respects to \\(\\hat{w}\\). Eventually we want to solve for \\(\\hat{w}\\). The equation below is the explicit formula for \\(\\hat{w}\\).\n\nIn our LinearRegression class, we implemented a fit_analytic method using this explicit formula for \\(\\hat{w}\\). We also implemented a fit_gradient method, which is gradient descent for linear regression. To implement fit_gradient, we used the formula\n\nWith each weight update, the gradient multiplied by the alpha value (default of 0.01) was subtracted from the weight to obtain a new weight.\n\n\n\n\nBefore starting, you need to load all of these libraries, modules, and functions.\n\nfrom linearRegression import LinearRegression\nimport numpy as np\nfrom matplotlib import pyplot as plt\nfrom sklearn.linear_model import Lasso\n\ndef pad(X):\n    return np.append(X, np.ones((X.shape[0], 1)), 1)\n\ndef LR_data(n_train = 100, n_val = 100, p_features = 1, noise = .1, w = None):\n    if w is None: \n        w = np.random.rand(p_features + 1) + .2\n    \n    X_train = np.random.rand(n_train, p_features)\n    y_train = pad(X_train)@w + noise*np.random.randn(n_train)\n\n    X_val = np.random.rand(n_val, p_features)\n    y_val = pad(X_val)@w + noise*np.random.randn(n_val)\n    \n    return X_train, y_train, X_val, y_val\n\n\n\n\n\nTo demonstrate that we’ve implemented LinearRegression correctly, we are going to test it on a data set that has 100 data points and 1 feature.\n\nn_train = 100\nn_val = 100\np_features = 1\nnoise = 0.2\n\n# create some data\nX_train, y_train, X_val, y_val = LR_data(n_train, n_val, p_features, noise)\n\n# plot it\nfig, axarr = plt.subplots(1, 2, sharex = True, sharey = True)\naxarr[0].scatter(X_train, y_train)\naxarr[1].scatter(X_val, y_val)\nlabs = axarr[0].set(title = \"Training\", xlabel = \"x\", ylabel = \"y\")\nlabs = axarr[1].set(title = \"Validation\", xlabel = \"x\")\nplt.tight_layout()\n\n\n\n\nHere we fit our model using the class we implemented and test out the fit_analytic method.\n\nLR = LinearRegression()\nLR.fit_analytic(X_train, y_train) # I used the analytical formula as my default fit method\n\nprint(f\"Training score = {LR.score(pad(X_train), y_train).round(4)}\")\nprint(f\"Validation score = {LR.score(pad(X_val), y_val).round(4)}\")\n\nTraining score = 0.6965\nValidation score = 0.7433\n\n\nWe can check the estimated weight vector w\n\nLR.w\n\narray([1.11225311, 1.22342203])\n\n\nWe can use the fit_gradient method we implemented to get the same value for w as well.\n\nLR2 = LinearRegression()\n\nLR2.fit_gradient(X_train, y_train, alpha = 0.01, max_iter = 1e2)\nLR2.w\n\narray([1.11236121, 1.22336123])\n\n\nHere we plot the change in score over time.\n\nplt.plot(LR2.score_history)\nlabels = plt.gca().set(xlabel = \"Iteration\", ylabel = \"Score\")\n\n\n\n\n\n\n\nBelow we’ll be looking at an experiment in which the number of features used, p_features, will increase while the number of training points, n_train, stays constant.\n\nn_train = 100\nn_val = 100\np_features = 1\nnoise = 0.2\n\ntraining_scores = []\nvalidation_scores = []\n\nwhile (p_features < n_train):\n    # create some data\n    X_train, y_train, X_val, y_val = LR_data(n_train, n_val, p_features, noise)\n    \n    LR = LinearRegression()\n    \n    LR.fit_analytic(X_train, y_train)\n    \n    train_scores = LR.score(pad(X_train), y_train)\n    val_scores = LR.score(pad(X_val), y_val)\n    \n    training_scores.append(train_scores)\n    validation_scores.append(val_scores)\n    \n    p_features += 1\n    \nplt.plot(training_scores, label = \"Training\")\nplt.plot(validation_scores, label = \"Validation\")\n\nplt.xscale('log')\nlegend = plt.legend() \nlabels = plt.gca().set(xlabel = \"# Features\", ylabel = \"Score\")\n\n\n\n\nYou can see in the graph above that as the number of features used increased, the training score also increased. The validation score, however, experienced a lot of oscilation as the number of features increased and eventually the score dropped by the end. This is an indication of overfitting since the difference between our training score and validation score increased a lot with the number of features.\n\n\n\nBelow are some experiments done using the LASSO algorithm from scikit-learn. This algorithm has a modified loss function with a regularization term, which makes the entries of the weight vector w small. It tends to force entries of the weight vector to be exactly zero.\n\nn_train = 100\nn_val = 100\np_features = 1\nnoise = 0.2\n\ntraining_scores = []\nvalidation_scores = []\n\nwhile (p_features < n_train):\n    # create some data\n    X_train, y_train, X_val, y_val = LR_data(n_train, n_val, p_features, noise)\n    \n    L = Lasso(alpha = 0.001)\n    \n    L.fit(X_train, y_train)\n    \n    train_scores = L.score(X_train, y_train)\n    val_scores = L.score(X_val, y_val)\n    \n    training_scores.append(train_scores)\n    validation_scores.append(val_scores)\n    \n    p_features += 1\n    \nplt.plot(training_scores, label = \"Training\")\nplt.plot(validation_scores, label = \"Validation\")\n\nplt.xscale('log')\nlegend = plt.legend() \nlabels = plt.gca().set(xlabel = \"# Features\", ylabel = \"Score\")\n\n\n\n\nIn this first experiment, we are working with 100 data points and 1 feature. The only difference between this and the previous experiment (Linear Regression) is the algorithm. With an alpha value of 0.001, the training accuracy increases with the number of features, but the validation accuracy decreases. We can see that the model was overfitting towards the end. These results are similar to the standard linear regression version.\n\nn_train = 100\nn_val = 100\np_features = 1\nnoise = 0.2\n\ntraining_scores = []\nvalidation_scores = []\n\nwhile (p_features < n_train):\n    # create some data\n    X_train, y_train, X_val, y_val = LR_data(n_train, n_val, p_features, noise)\n    \n    L = Lasso(alpha = 0.1)\n    \n    L.fit(X_train, y_train)\n    \n    train_scores = L.score(X_train, y_train)\n    val_scores = L.score(X_val, y_val)\n    \n    training_scores.append(train_scores)\n    validation_scores.append(val_scores)\n    \n    p_features += 1\n    \nplt.plot(training_scores, label = \"Training\")\nplt.plot(validation_scores, label = \"Validation\")\n\nplt.xscale('log')\nlegend = plt.legend() \nlabels = plt.gca().set(xlabel = \"# Features\", ylabel = \"Score\")\n\n\n\n\nIn this second experiment, we increased the alpha value to 0.1. With this increase, there is a lot more oscilation in both training and validation accuracy. Due to these oscilations, it seems that the training accuracy was not able to reach a perfect score of 1.0, like we observed in the linear regression version. The validation doesn’t really dip either, though that’s only because it never steadily increased in the first place.\n\n\nn_train = 100\nn_val = 100\np_features = 1\nnoise = 0.2\n\ntraining_scores = []\nvalidation_scores = []\n\nwhile (p_features < n_train + 999):\n    # create some data\n    X_train, y_train, X_val, y_val = LR_data(n_train, n_val, p_features, noise)\n    \n    L = Lasso(alpha = 0.1)\n    \n    L.fit(X_train, y_train)\n    \n    train_scores = L.score(X_train, y_train)\n    val_scores = L.score(X_val, y_val)\n    \n    training_scores.append(train_scores)\n    validation_scores.append(val_scores)\n    \n    p_features += 1\n    \nplt.plot(training_scores, label = \"Training\")\nplt.plot(validation_scores, label = \"Validation\")\n\nplt.xscale('log')\nlegend = plt.legend() \nlabels = plt.gca().set(xlabel = \"# Features\", ylabel = \"Score\")\n\n\n\n\nIn this final experiment, the alpha value is set to 0.1 and the number of feature is going post the number of data points we have. You can see the oscilation that was present in the previous experiment, but there’s more of a distinction in training scores and validation scores that’s reminiscent of the linear regression version."
  },
  {
    "objectID": "posts/auditing-bias/Untitled.html",
    "href": "posts/auditing-bias/Untitled.html",
    "title": "My Awesome CSCI 0451 Blog",
    "section": "",
    "text": "possible_features=['AGEP', 'SCHL', 'MAR', 'RELP', 'DIS', 'ESP', 'CIT', 'MIG', 'MIL', 'ANC', 'NATIVITY', 'DEAR', 'DEYE', 'DREM', 'SEX', 'RAC1P', 'ESR']\nacs_data[possible_features].head()\n\n\n\n\n\n  \n    \n      \n      AGEP\n      SCHL\n      MAR\n      RELP\n      DIS\n      ESP\n      CIT\n      MIG\n      MIL\n      ANC\n      NATIVITY\n      DEAR\n      DEYE\n      DREM\n      SEX\n      RAC1P\n      ESR\n    \n  \n  \n    \n      0\n      19\n      18.0\n      5\n      17\n      2\n      NaN\n      1\n      3.0\n      4.0\n      1\n      1\n      2\n      2\n      2.0\n      2\n      1\n      6.0\n    \n    \n      1\n      18\n      18.0\n      5\n      17\n      2\n      NaN\n      1\n      3.0\n      4.0\n      1\n      1\n      2\n      2\n      2.0\n      2\n      2\n      6.0\n    \n    \n      2\n      53\n      17.0\n      5\n      16\n      1\n      NaN\n      1\n      1.0\n      4.0\n      2\n      1\n      2\n      2\n      1.0\n      1\n      1\n      6.0\n    \n    \n      3\n      28\n      19.0\n      5\n      16\n      2\n      NaN\n      1\n      1.0\n      2.0\n      1\n      1\n      2\n      2\n      2.0\n      1\n      1\n      6.0\n    \n    \n      4\n      25\n      12.0\n      5\n      16\n      1\n      NaN\n      1\n      3.0\n      4.0\n      1\n      1\n      2\n      2\n      1.0\n      2\n      1\n      6.0\n    \n  \n\n\n\n\n\nfeatures_to_use = [f for f in possible_features if f not in [\"ESR\", \"RAC1P\"]]\n\n\nEmploymentProblem = BasicProblem(\n    features=features_to_use,\n    target='ESR',\n    target_transform=lambda x: x == 1,\n    group='RAC1P',\n    preprocess=lambda x: x,\n    postprocess=lambda x: np.nan_to_num(x, -1),\n)\n\nfeatures, label, group = EmploymentProblem.df_to_numpy(acs_data)\n\n\nfor obj in [features, label, group]:\n  print(obj.shape)\n\n(47777, 15)\n(47777,)\n(47777,)\n\n\n\nfrom sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test, group_train, group_test = train_test_split(\n    features, label, group, test_size=0.2, random_state=0)\n\n\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.pipeline import make_pipeline\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.metrics import confusion_matrix\n\nmodel = make_pipeline(StandardScaler(), LogisticRegression())\nmodel.fit(X_train, y_train)\n\nPipeline(steps=[('standardscaler', StandardScaler()),\n                ('logisticregression', LogisticRegression())])In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.PipelinePipeline(steps=[('standardscaler', StandardScaler()),\n                ('logisticregression', LogisticRegression())])StandardScalerStandardScaler()LogisticRegressionLogisticRegression()\n\n\n\ny_hat = model.predict(X_test)\n\n\n(y_hat == y_test).mean()\n\n0.7842193386354123\n\n\n\n(y_hat == y_test)[group_test == 1].mean()\n\n0.7838255977496483\n\n\n\n(y_hat == y_test)[group_test == 2].mean()\n\n0.7838630806845965"
  },
  {
    "objectID": "posts/palmer-penguins/palmer_penguins.html",
    "href": "posts/palmer-penguins/palmer_penguins.html",
    "title": "My Awesome CSCI 0451 Blog",
    "section": "",
    "text": "In this blog post, we’ll walk through a complete example of the standard machine learning workflow using the Palmer Penguins data set. The main objective is to determine a set of features that can be used to correctly determine the species of a penguin.\n\n\n\n\nBefore starting, you need to load all of these libraries and modules:\n\nimport pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport numpy as np\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.tree import DecisionTreeClassifier \nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.svm import SVC \nfrom itertools import combinations\nfrom sklearn.model_selection import cross_val_score\nfrom matplotlib.patches import Patch\n\n\n\n\n\nHere we are loading in the data and preparing the qualitive columns in the data set.\n\ntrain_url = \"https://raw.githubusercontent.com/middlebury-csci-0451/CSCI-0451/main/data/palmer-penguins/train.csv\"\ntrain = pd.read_csv(train_url)\n\n#train.head()\n\nle = LabelEncoder()\nle.fit(train[\"Species\"])\n\ndef prepare_data(df):\n  df = df.drop([\"studyName\", \"Sample Number\", \"Individual ID\", \"Date Egg\", \"Comments\", \"Region\"], axis = 1)\n  # Remove any rows where the value in the \"Sex\" column is \".\"\n  df = df[df[\"Sex\"] != \".\"]\n  df = df.dropna()\n  y = le.transform(df[\"Species\"])\n  df = df.drop([\"Species\"], axis = 1)\n  df = pd.get_dummies(df)\n  return df, y\n\nX_train, y_train = prepare_data(train)\n\n#X_train\n\n\n\n\nTo learn a little more about the data set, we grouped the data by sex and body mass (g). A 1 in the Sex_MALE column represents the male penguins, and the 0 represents the female penguins. As you can see, the body mass differs greatly between the two sex (~800 g). While this is information is not needed for species classification (what we are trying to do in this blog post), this is interesting information for a penguin sex classification.\n\ndata = X_train.groupby('Sex_MALE')['Body Mass (g)'].mean()\ndf = pd.DataFrame(data)\ndf\n\n\n\n\n\n  \n    \n      \n      Body Mass (g)\n    \n    \n      Sex_MALE\n      \n    \n  \n  \n    \n      0\n      3823.214286\n    \n    \n      1\n      4613.076923\n    \n  \n\n\n\n\nWe also looked into the possible relationship between culmen length (mm) and body mass (g) on each island (Island Dream, Island Biscoe, and Island Torgersen). While these aren’t the best graphs, you can see that there’s a big difference between the culmen length (mm) and body mass (g) on each island. The graph for Island Biscoe shows a very linear relationship between culmen length (mm) and body mass (g). Could this be a sign that Island Biscoe has a more homogeneous penguin (species) population? It’s also interesting to note that the penguins on Island Torgersen seem to have smaller culmen length (mm) and body mass (g) than the other two islands. However, it also seems to be more varied– not as clear of a relationship. Does this mean that Island Torgersen has a more varied penguin (species) population?\n\n# Generate the three plots\ng1 = sns.lmplot(\n    data = X_train,\n    x = \"Culmen Length (mm)\",\n    y = \"Body Mass (g)\",\n    hue = \"Island_Dream\"\n)\ng2 = sns.lmplot(\n    data = X_train,\n    x = \"Culmen Length (mm)\",\n    y = \"Body Mass (g)\",\n    hue = \"Island_Biscoe\"\n)\ng3 = sns.lmplot(\n    data = X_train,\n    x = \"Culmen Length (mm)\",\n    y = \"Body Mass (g)\",\n    hue = \"Island_Torgersen\"\n)\n\n# Add titles to the subplots\ng1.ax.set_title(\"Island_Dream\")\ng2.ax.set_title(\"Island_Biscoe\")\ng3.ax.set_title(\"Island_Torgersen\")\n\nText(0.5, 1.0, 'Island_Torgersen')\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nHere we are choosing the features (at least one qualitative and at least two quantitative). To do so, we are using the exhaustive search approach to go through all the features contained in the data set. We define two lists that contains the names of the qualitative and quantitative data in the data set, and we create a pandas dataframe that would later store the results of the cross-validation evaluation for each combination of features. We iterate through the two lists we created. We create a new LogisticRegression object and we use cross-validation to evaluate the performance of the logistic regression model using the current combination of features.\n\n#selecting the columns that we want\nall_qual_cols = [\"Clutch Completion\", \"Sex\", \"Island\", \"Stage\"]\nall_quant_cols = ['Culmen Length (mm)', 'Culmen Depth (mm)', 'Flipper Length (mm)', 'Body Mass (g)']\n\nscores_df = pd.DataFrame(columns=['Columns', 'Score'])\n\nfor qual in all_qual_cols: \n    qual_cols = [col for col in X_train.columns if qual in col]\n  \n    for pair in combinations(all_quant_cols, 2):\n        cols = qual_cols + list(pair)  \n\n        lr = LogisticRegression(max_iter = 10000) # included the max number of iterations because I kept getting warnings\n        \n        cv_mean = cross_val_score(lr, X_train[cols], y_train, cv = 10).mean()\n        scores_df = pd.concat([scores_df, pd.DataFrame({'Columns': cols, 'Score': cv_mean})], ignore_index=True)\n\nscores_df = scores_df.sort_values(by='Score', ascending=False).reset_index(drop=True) # sort score in decreasing order\n\nscores_df.head()\nfeatures = ['Culmen Length (mm)', 'Culmen Depth (mm)', 'Island_Biscoe', 'Island_Dream', 'Island_Torgersen']\n\nAfter choosing our features, we train our model with those features. We get a 0.996 or 99.6% training accuracy. We then test the model and get a 1.0 or 100% testing accuracy.\n\nlr = LogisticRegression(max_iter = 1000)\nlr.fit(X_train[features], y_train)\nlr.score(X_train[features], y_train)\n\n0.99609375\n\n\n\ntest_url = \"https://raw.githubusercontent.com/middlebury-csci-0451/CSCI-0451/main/data/palmer-penguins/test.csv\"\ntest = pd.read_csv(test_url)\n\nX_test, y_test = prepare_data(test)\n\nlr.score(X_test[features], y_test)\n\n1.0\n\n\nWe also tried training on a RandomForestClassifier model.\n\nrf = RandomForestClassifier()\nrf.fit(X_train[features], y_train)\nrf.score(X_train[features], y_train)\n\n1.0\n\n\n\nrf.score(X_test[features], y_test)\n\n0.9852941176470589\n\n\n\n\n\nUsing the follow function, we will show the decision regions of the finished models.\n\ndef plot_regions(model, X, y):\n    \n    x0 = X[X.columns[0]]\n    x1 = X[X.columns[1]]\n    qual_features = X.columns[2:]\n    \n    fig, axarr = plt.subplots(1, len(qual_features), figsize = (7, 3))\n\n    # create a grid\n    grid_x = np.linspace(x0.min(),x0.max(),501)\n    grid_y = np.linspace(x1.min(),x1.max(),501)\n    xx, yy = np.meshgrid(grid_x, grid_y)\n    \n    XX = xx.ravel()\n    YY = yy.ravel()\n\n    for i in range(len(qual_features)):\n        XY = pd.DataFrame({\n          X.columns[0] : XX,\n          X.columns[1] : YY\n      })\n\n        for j in qual_features:\n            XY[j] = 0\n            \n        XY[qual_features[i]] = 1\n            \n        p= model.predict(XY)\n        p = p.reshape(xx.shape)\n      \n      \n        # use contour plot to visualize the predictions\n        axarr[i].contourf(xx, yy, p, cmap = \"jet\", alpha = 0.2, vmin = 0, vmax = 2)\n      \n        ix = X[qual_features[i]] == 1\n        # plot the data\n        axarr[i].scatter(x0[ix], x1[ix], c = y[ix], cmap = \"jet\", vmin = 0, vmax = 2)\n      \n        axarr[i].set(xlabel = X.columns[0], \n            ylabel  = X.columns[1])\n      \n        patches = []\n        for color, spec in zip([\"red\", \"green\", \"blue\"], [\"Adelie\", \"Chinstrap\", \"Gentoo\"]):\n            patches.append(Patch(color = color, label = spec))\n\n        plt.legend(title = \"Species\", handles = patches, loc = \"best\")\n      \n        plt.tight_layout()\n\nThese are the decision regions for the LogisticRegression model. As you can see, the model did a really good job at classifying the three penguin species.\n\nplot_regions(lr, X_train[features], y_train)\n\n\n\n\nThese are the decision regions for the RandomForestClassifier model. There may be a little bit of overfitting because of the jagged boundaries, however the model did a pretty good job at classifying the species.\n\nplot_regions(rf, X_train[features], y_train)"
  },
  {
    "objectID": "posts/auditing-bias/auditing-bias.html",
    "href": "posts/auditing-bias/auditing-bias.html",
    "title": "My Awesome CSCI 0451 Blog",
    "section": "",
    "text": "In this blog post, we aim to create a machine learning model that predicts whether a person’s income is over $50k using their demographic data, excluding sex. Once we have a working model, we will perform a fairness audit in order to assess whether or not the algorithm displays bias for demographic characteristics like sex.\n\n\n\n\nBefore starting, you’ll want to import the following libraries and modules. We’ll also be using the folktables package to download and organize data from the American Community Survey’s Public Use Microdata Sample (PUMS).\n\nfrom folktables import ACSDataSource, ACSEmployment, BasicProblem, adult_filter\nimport numpy as np\nfrom sklearn.model_selection import train_test_split\nimport pandas as pd\nimport seaborn as sns\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.preprocessing import PolynomialFeatures, StandardScaler\nfrom sklearn.metrics import confusion_matrix\n\n\n\n\n\nFirst thing we want to do is get the data for the state that we want to work with. The state we’ll be working with in this blog post is Mississippi (MS) because it has the one of the highest poverty rates in the country. However, any state would work as well. We add a column at the end of the data frame to indicate whether the indiviual has an income of over 50k.\n\nSTATE = \"MS\"\n\ndata_source = ACSDataSource(survey_year='2018', \n                            horizon='1-Year', \n                            survey='person')\n\nacs_data = data_source.get_data(states=[STATE], download=True)\n\n#add column to indicate whether an individual is employed\nacs_data = acs_data.assign(S50K = 1 * (acs_data['PINCP'] > 50000)) \n\nacs_data.head()\n\n\n\n\n\n  \n    \n      \n      RT\n      SERIALNO\n      DIVISION\n      SPORDER\n      PUMA\n      REGION\n      ST\n      ADJINC\n      PWGTP\n      AGEP\n      ...\n      PWGTP72\n      PWGTP73\n      PWGTP74\n      PWGTP75\n      PWGTP76\n      PWGTP77\n      PWGTP78\n      PWGTP79\n      PWGTP80\n      S50K\n    \n  \n  \n    \n      0\n      P\n      2018GQ0000052\n      6\n      1\n      1700\n      3\n      28\n      1013097\n      42\n      51\n      ...\n      80\n      76\n      81\n      79\n      43\n      44\n      80\n      78\n      5\n      0\n    \n    \n      1\n      P\n      2018GQ0000125\n      6\n      1\n      1100\n      3\n      28\n      1013097\n      50\n      64\n      ...\n      45\n      43\n      77\n      39\n      6\n      8\n      6\n      50\n      6\n      0\n    \n    \n      2\n      P\n      2018GQ0000187\n      6\n      1\n      300\n      3\n      28\n      1013097\n      11\n      93\n      ...\n      0\n      12\n      0\n      25\n      12\n      0\n      3\n      14\n      12\n      0\n    \n    \n      3\n      P\n      2018GQ0000190\n      6\n      1\n      600\n      3\n      28\n      1013097\n      38\n      68\n      ...\n      46\n      41\n      39\n      39\n      72\n      41\n      3\n      73\n      73\n      0\n    \n    \n      4\n      P\n      2018GQ0000204\n      6\n      1\n      1300\n      3\n      28\n      1013097\n      21\n      34\n      ...\n      4\n      4\n      37\n      19\n      2\n      40\n      39\n      22\n      20\n      0\n    \n  \n\n5 rows × 287 columns\n\n\n\nAs you can see from the data above, there are over 280 features that we can work with. 280 features are a lot, so we will be focusing on the features below:\nAGEP: Age\nCOW: Class of worker\nSCHL: Educational attainment\nMAR: Marital status\nOCCP: Occupation\nPOBP: Place of birth\nRELP: Relationship\nWKHP: Usual hours worked per week past 12 months\nRAC1P: Recoded detailed race code\nDIS: Disability recode\nCIT: Citizenship status\nMIG: Mobility status (lived here 1 year ago)\nMIL: Military service\nDEAR: Hearing difficulty\nDEYE: Vision difficulty\nDREM: Cognitive difficulty\nESR: Employment status recode\n\np_feat = ['AGEP', 'COW', 'SCHL', 'MAR', 'OCCP', 'POBP', 'RELP', 'WKHP', 'RAC1P', 'DIS', 'MIG', 'CIT', 'MIL', 'NATIVITY', 'DEAR', 'DEYE', 'DREM', 'SEX', 'PINCP', 'S50K']\nacs_data[p_feat].head()\n\n\n\n\n\n  \n    \n      \n      AGEP\n      COW\n      SCHL\n      MAR\n      OCCP\n      POBP\n      RELP\n      WKHP\n      RAC1P\n      DIS\n      MIG\n      CIT\n      MIL\n      NATIVITY\n      DEAR\n      DEYE\n      DREM\n      SEX\n      PINCP\n      S50K\n    \n  \n  \n    \n      0\n      51\n      1.0\n      20.0\n      3\n      4760.0\n      28\n      16\n      NaN\n      2\n      1\n      1.0\n      1\n      4.0\n      1\n      2\n      2\n      2.0\n      1\n      0.0\n      0\n    \n    \n      1\n      64\n      NaN\n      16.0\n      3\n      NaN\n      28\n      17\n      NaN\n      1\n      1\n      1.0\n      1\n      4.0\n      1\n      2\n      2\n      1.0\n      1\n      15000.0\n      0\n    \n    \n      2\n      93\n      NaN\n      5.0\n      2\n      NaN\n      28\n      16\n      NaN\n      2\n      1\n      1.0\n      1\n      4.0\n      1\n      2\n      2\n      2.0\n      1\n      6700.0\n      0\n    \n    \n      3\n      68\n      NaN\n      1.0\n      5\n      NaN\n      28\n      16\n      NaN\n      1\n      1\n      1.0\n      1\n      4.0\n      1\n      2\n      2\n      1.0\n      1\n      5800.0\n      0\n    \n    \n      4\n      34\n      NaN\n      1.0\n      5\n      NaN\n      28\n      17\n      NaN\n      2\n      1\n      1.0\n      1\n      4.0\n      1\n      2\n      2\n      1.0\n      1\n      530.0\n      0\n    \n  \n\n\n\n\nWe will not include SEX, PINCP, and S50K, since our group choice is sex (we will evaluate bias against) and salary is our target variable.\n\nfeatures_to_use = [f for f in p_feat if f not in ['SEX', 'PINCP', 'S50K']]\n\nNow that we have our features, we will construct a BasicProblem that will allow us to use these features to predict whether an individual makes over 50k. SEX will be our group label, and S50K will be our target.\n\nSalaryProblem = BasicProblem(\n    features=features_to_use,\n    target='S50K',\n    target_transform=lambda x: x == 1,\n    group='SEX',\n    preprocess=lambda x: x,\n    postprocess=lambda x: np.nan_to_num(x, -1),\n)\n\nfeatures, label, group = SalaryProblem.df_to_numpy(acs_data)\n\nThe result is a feature matrix features in a format that we can work with\n\nfor obj in [features, label, group]:\n  print(obj.shape)\n\n(29124, 17)\n(29124,)\n(29124,)\n\n\nWe now want to split the data using train_test_split.\n\nX_train, X_test, y_train, y_test, group_train, group_test = train_test_split(\n    features, label, group, test_size=0.2, random_state=0)\n\n\n\n\n\nWe will use simple descriptive analysis to learn more about our data. We first want to inspect how many individuals we’ll be working with.\n\ndf = pd.DataFrame(X_train, columns = features_to_use)\ndf['SEX'] = group_train\ndf['>50k'] = y_train\n\ndf.shape\n\n(23299, 19)\n\n\nBased on the dimensions of our data frame, we will have 23299 individuals. We then want to figure out the proportion of individuals who have an income of over 50k.\n\nover_50k = df.groupby(['>50k']).size().reset_index(name = 'count')\nover_50k['%'] = (over_50k['count'] / len(df.index) * 100).round(2)\n\nover_50k    \n\n\n\n\n\n  \n    \n      \n      >50k\n      count\n      %\n    \n  \n  \n    \n      0\n      False\n      19955\n      85.65\n    \n    \n      1\n      True\n      3344\n      14.35\n    \n  \n\n\n\n\nHere we can see that only 3,344 people out of 23,299 in Mississippi (14.35%) have an income over 50k. Next, we want to inspect how many of this 14.35% are males and how many are females.\n\nf_df = df[df['>50k'] == True]\n\nover_50k_s = f_df.groupby(['SEX']).size().reset_index(name = 'count')\nover_50k_s['%'] = ((over_50k_s['count'] / len(f_df.index)) * 100).round(2)\n\nover_50k_s\n\n\n\n\n\n  \n    \n      \n      SEX\n      count\n      %\n    \n  \n  \n    \n      0\n      1\n      2213\n      66.18\n    \n    \n      1\n      2\n      1131\n      33.82\n    \n  \n\n\n\n\nFrom our data frame, we learn that of the 14.35% who are making over 50k, about 66.18% of them are males while 33.82% are females. Next, we want to look at the separate sex groups– we want to find out the proportion of males who are making over 50k, and the proportion of females who are making over 50k.\n\nf_df = df[df['SEX'] == 1]\n\nmale = f_df.groupby(['>50k']).size().reset_index(name = 'count')\nmale['percent'] = ((male['count'] / len(f_df.index)) * 100).round(2)\n\nmale\n\n\n\n\n\n  \n    \n      \n      >50k\n      count\n      percent\n    \n  \n  \n    \n      0\n      False\n      8857\n      80.01\n    \n    \n      1\n      True\n      2213\n      19.99\n    \n  \n\n\n\n\nThe table above is of the male group. About 20% of males make over 50k.\n\nf_df = df[df['SEX'] == 2]\n\nfemale = f_df.groupby(['>50k']).size().reset_index(name = 'count')\nfemale['percent'] = ((female['count'] / len(f_df.index)) * 100).round(2)\n\nfemale\n\n\n\n\n\n  \n    \n      \n      >50k\n      count\n      percent\n    \n  \n  \n    \n      0\n      False\n      11098\n      90.75\n    \n    \n      1\n      True\n      1131\n      9.25\n    \n  \n\n\n\n\nThe table above is of the female group. About 9.25% of females make over 50k. We will also check for intersectional trends by computing the proportion of positive labels by both sex and race.\n\nf_df = df[df['>50k'] == 1]\n\ninter = f_df.groupby(['SEX', 'RAC1P']).size().reset_index(name = 'count')\ninter['%'] = ((inter['count'] / len(f_df.index)) * 100).round(2)\n\ninter\n\n\n\n\n\n  \n    \n      \n      SEX\n      RAC1P\n      count\n      %\n    \n  \n  \n    \n      0\n      1\n      1.0\n      1811\n      54.16\n    \n    \n      1\n      1\n      2.0\n      335\n      10.02\n    \n    \n      2\n      1\n      3.0\n      6\n      0.18\n    \n    \n      3\n      1\n      6.0\n      26\n      0.78\n    \n    \n      4\n      1\n      8.0\n      13\n      0.39\n    \n    \n      5\n      1\n      9.0\n      22\n      0.66\n    \n    \n      6\n      2\n      1.0\n      872\n      26.08\n    \n    \n      7\n      2\n      2.0\n      232\n      6.94\n    \n    \n      8\n      2\n      3.0\n      1\n      0.03\n    \n    \n      9\n      2\n      6.0\n      15\n      0.45\n    \n    \n      10\n      2\n      8.0\n      1\n      0.03\n    \n    \n      11\n      2\n      9.0\n      10\n      0.30\n    \n  \n\n\n\n\n\nsns.set_style('whitegrid')\nsns.barplot(x = 'RAC1P', y = '%', hue = 'SEX', data = inter).set(title = 'Percentage of People with Income >50k by Sex and Race')\n\n[Text(0.5, 1.0, 'Percentage of People with Income >50k by Sex and Race')]\n\n\n\n\n\nWe created a table and a bar plot to visualize the percentage of people with income of over 50k grouped by their sex and race. Based on the graph, we can see that over 50% of white males make more than 50k. This is followed by white females, black or African American males, and black or African American females.\n\n\n\n\nFor our model, we’ll be using DecisionTreeClassifier. Since this model requires a specified max_depth, we will use cross validation to find it. After finding the max_depth, we will use it to create our model.\n\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.tree import DecisionTreeClassifier\nfrom matplotlib import pyplot as plt\n\nimport matplotlib.pyplot as plt\n\nplt.figure()\n\nmax_score = 0\nbest_depth = 0\nfor d in range(2, 20):\n    T = DecisionTreeClassifier(max_depth=d)\n    cv_mean = cross_val_score(T, X_train, y_train, cv=10).mean()\n    plt.plot(d, cv_mean, 'bo')\n    if cv_mean > max_score:\n        max_score = cv_mean\n        best_depth = d\n\nplt.xlabel('Complexity (depth)')\nplt.ylabel('Performance (score)')\nplt.show()\n\n\n\n\n\nfrom sklearn.pipeline import make_pipeline\nfrom sklearn.preprocessing import StandardScaler\n\ndt = DecisionTreeClassifier(max_depth = best_depth)\nmodel = make_pipeline(StandardScaler(), dt)\n\nmodel.fit(X_train, y_train)\n\nPipeline(steps=[('standardscaler', StandardScaler()),\n                ('decisiontreeclassifier',\n                 DecisionTreeClassifier(max_depth=6))])In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.PipelinePipeline(steps=[('standardscaler', StandardScaler()),\n                ('decisiontreeclassifier',\n                 DecisionTreeClassifier(max_depth=6))])StandardScalerStandardScaler()DecisionTreeClassifierDecisionTreeClassifier(max_depth=6)\n\n\nWe will define a function that’ll compute the positive predictive values (PPV), false negative rate (FNR), and false positive rate (FPR).\n\ndef PPV_FNR_FPR(data, predictions):\n    confusion = confusion_matrix(data, predictions)\n    TP = confusion[1][1]\n    TN = confusion[0][0]\n    FP = confusion[0][1]\n    FN = confusion[1][0]\n    PPV = TP / (TP + FP)\n    print(f\"{PPV=}\")\n    FNR = FN / (FN + TP)\n    print(f\"{FNR=}\")\n    FPR = FP / (FP + TN)\n    print(f\"{FPR=}\")\n\n\ny_hat = model.predict(X_test)\n\n\n\n\n\nWe want to perform an audit to address the overall measures:\n1. What is the overall accuracy of our model?\n2. What is the positive predictive value (PPV) of our model?\n3. What are the overall false negative and false positive rates (FNR and FPR) of our model?\nas well as the by-group measures:\n1. What is the accuracy of our model on each subgroup?\n2. What is the PPV of our model on each subgroup?\n3. What are the FNR and FPR on each subgroup?\n\n# Overall accuracy\n(y_hat == y_test).mean()\n\n0.8817167381974249\n\n\n\n# PPV, FNR, and FPR (overall)\nPPV_FNR_FPR(y_test, y_hat)\n\nPPV=0.6381818181818182\nFNR=0.582639714625446\nFPR=0.03992776886035313\n\n\n\n# Accuracy for male group\n(y_hat == y_test)[group_test == 1].mean()\n\n0.8524822695035461\n\n\n\n# PPV, FNR, and FPR for male group\nPPV_FNR_FPR(y_test[group_test == 1], y_hat[group_test == 1])\n\nPPV=0.7366666666666667\nFNR=0.6039426523297491\nFPR=0.034924845269672856\n\n\n\n# Acuracy for female group\n(y_hat == y_test)[group_test == 2].mean()\n\n0.9091514143094842\n\n\n\n# PPV, FNR, and FPR for female group\nPPV_FNR_FPR(y_test[group_test == 2], y_hat[group_test == 2])\n\nPPV=0.52\nFNR=0.5406360424028268\nFPR=0.0440852314474651\n\n\nIt does not seem like our model is calibrated because the PPV is not consistent across the different groups. It also seems like the model does not satify approximate error rate balance because of the big difference between FNR and FPR. It seems that for all groups, the model is prioritzing the minimization of false positives over false negatives. Our model also doesn’t satisfy the statistical parity because of the differences in PPV, FNR, and FPR across all groups.\n\n\n\n\nThis model could potentially benefit researchers and sociologists who are studying income inequality, social mobility, or demographic patterns as they can get insights into the relationship between demographics and income. Financial institutions (e.g. banks, lenders, etc.) and different governments can also potentially benefit from this as they can use it to assess credit worthiness and make decisions regarding loan approvals, interest rates, or financial services.\nHowever, this could affect other people’s quality lives. For this model specifically, the FNR for all three groups (overall, male, and females) is around 0.57, which is relatively high. This also indicates that the model is incorrectly predicting postive as negative. This could lead to those who don’t really need extra financial assistance to be falsely classified as needing the assistance, taking the opportunity away from those actually in need.\nOn a similar note, the model does display problematic bias considering that it failed to satisfy approximate error rate balance and statistical parity. It is also not calibrated. Using the example above, the FNR for the male group is higher than the female group. This could result in someone from the male group getting the extra financial assistance when they may not need it as much compared to someone from the female group (higher FPR than the male group – incorrectly classifying them as having a higher income than they may have).\nBeside bias, a model like this is also concerning because of the data behind it. This model predicts a person’s income based on their demographics, which means that it may involve making inferences about an individual’s financial situation without their explicit consent or knowledge. Another point is that this model uses about 17 features, however there are more factors that can influence a person’s income. By not including those other factors, this model would not accurately predict an individual’s income. A possible way to address these problems would be a more robust data collection process that involves consent. In addition to that, we’d need to make sure the data is comprehensive and diverse, covering a wide range of factors that influence income, including both demographic and non-demographic variables."
  }
]