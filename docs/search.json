[
  {
    "objectID": "posts/example-blog-post/index.html",
    "href": "posts/example-blog-post/index.html",
    "title": "Hello Blog",
    "section": "",
    "text": "This is an example of the blog posts that you’ll submit as your primary form of learning demonstration in CSCI 0451. I created this post by modifying the file posts/example-blog-post/index.ipynb in VSCode. You can also use JupyterLab for this editing if you prefer. Finally, it is possible to write blog posts without using notebooks by writing .qmd files, as illustrated here."
  },
  {
    "objectID": "posts/example-blog-post/index.html#math",
    "href": "posts/example-blog-post/index.html#math",
    "title": "Hello Blog",
    "section": "Math",
    "text": "Math\nIn addition to regular text using the Markdown specification, you can also write mathematics, enclosed between dollar signs. The syntax for writing math is very similar to the syntax used in the \\(\\LaTeX\\) markup language. For example, $f(x) \\approx y$ renders to \\(f(x) \\approx y\\). To place complex mathematical expressions on their own lines, use double dollar signs. For example, the expression\n$$\\mathcal{L}(a, b) = \\sum_{i = 1}^n (ax_i + b - y_i)^2$$\nrenders to:\n\\[\\mathcal{L}(a, b) = \\sum_{i = 1}^n (ax_i + b - y_i)^2\\;.\\]\nBehind the scenes, math is powered by the MathJax engine. For more on how to write math, check this handy tutorial and quick reference."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "My Awesome CSCI 0451 Blog",
    "section": "",
    "text": "An example blog post illustrating the key techniques you’ll need to demonstrate your learning in CSCI 0451.\n\n\n\n\n\n\nJan 10, 2023\n\n\nPhil Chodrow\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "About this blog"
  },
  {
    "objectID": "posts/perceptron/perceptron.html",
    "href": "posts/perceptron/perceptron.html",
    "title": "My Awesome CSCI 0451 Blog",
    "section": "",
    "text": "Source code: perceptron.py\n\n\n\nThis blog post will cover the perceptron algorithm. This is a binary classification algorithm that takes in an input and finds a hyperplane that splits that input data such that all similar data points lie on either side of the hyperplane. This algorithm only works if the data is linearly separable.\n\n\n\n\nIn our Perceptron class, we implemented a fit(X, y) method that takes in an array of some data, X, and an array of labels, y, corresponding to the elements in the input data. Each element in y is either a 1 or a 0.\nIn the method, we started with a random value for weight, self.w, such that w = (w, -b). We also initalized a list, self.history, that would keep track of the scores over the training period. While the classification is not perfect (i.e., achieved a 100% accuracy rate) and the max number of iterations has not been reached, we will continue to update our w depending on if it was able to correctly classify a random data point, i, in our data set X. If it wasn’t able to correctly classify the random data point, we update w using the following equation:\n\nThis update is done in line 52 of our perceptron.py code. This update moves our separator in the right direction so that our randomly selected point could be correctly classified on the next iteration if it were to be selected again. As this loops continues, the weight is added to self.history.\n\n\n\n\nBefore starting, you need to load all of these libraries and modules:\n\nimport numpy as np\nimport pandas as pd\nimport seaborn as sns\nfrom matplotlib import pyplot as plt\nfrom perceptron import Perceptron\nfrom sklearn.datasets import make_blobs\n\n\n\n\nThis block of code generates a visualization that contains data that is linearly separable with 2 features.\n\nnp.random.seed(12345)\n\nn = 100\np_features = 3\n\nX, y = make_blobs(n_samples = 100, n_features = p_features - 1, centers = [(-1.7, -1.7), (1.7, 1.7)])\n\nfig = plt.scatter(X[:,0], X[:,1], c = y)\nxlab = plt.xlabel(\"Feature 1\")\nylab = plt.ylabel(\"Feature 2\")\n\n\n\n\nJust by looking at the graph, we can see that it is possible to have a line that separates all the purple “0” labels from the yellow “1” labels. To analyze it further, however, we can use the algorithm we implemented!\n\np = Perceptron()\np.fit(X, y, max_steps = 1000)\nfig = plt.plot(p.history)\nxlab = plt.xlabel(\"Iteration\")\nylab = plt.ylabel(\"Accuracy\")\n\n\n\n\nFrom the graph above, we can see that while there are variations in the accuracy, the algorithm does eventually achieved a perfect classification rate of 1.00. This means that the data was indeed separable and we were able to separate the data points into their respective labels.\n\ndef draw_line(w, x_min, x_max):\n  x = np.linspace(x_min, x_max, 101)\n  y = -(w[0]*x + w[2])/w[1]\n  plt.plot(x, y, color = \"black\")\n\nfig = plt.scatter(X[:,0], X[:,1], c = y)\nfig = draw_line(p.w, -2, 2)\n\nxlab = plt.xlabel(\"Feature 1\")\nylab = plt.ylabel(\"Feature 2\")\n\n\n\n\nHere is a visualization of the graph with a line separating the data points.\n\n\n\n\nThis block of code generates a visualization that contains data that is not linearly separable with 2 features.\n\nnp.random.seed(12345)\n\nn = 100\np_features = 3\n\nX, y = make_blobs(n_samples = 100, n_features = p_features - 1, centers = [(-1, -1), (1, 1)])\n\nfig = plt.scatter(X[:,0], X[:,1], c = y)\nxlab = plt.xlabel(\"Feature 1\")\nylab = plt.ylabel(\"Feature 2\")\n\n\n\n\n\np = Perceptron()\np.fit(X, y, max_steps = 1000)\nfig = plt.plot(p.history)\nxlab = plt.xlabel(\"Iteration\")\nylab = plt.ylabel(\"Accuracy\")\n\n\n\n\nHere we see the oscillation of accuracy. It never reaches 1.0 and the iterating stops once the max number of iterations is reached. This means the data is not linearly separable and the algorithm was not able to draw a line that would be able to separate the data points.\n\nfig = plt.scatter(X[:,0], X[:,1], c = y)\nfig = draw_line(p.w, -2, 2)\n\nxlab = plt.xlabel(\"Feature 1\")\nylab = plt.ylabel(\"Feature 2\")\n\n\n\n\nThis visualization shows that the algorithm was not able to find a line that would separate the data points accordingly.\n\n\n\n\nBelow is a block of code that generations a visualization of the evolution of the score over the training period of a random data set that contains 7 features.\n\nnp.random.seed(12345)\n\nn = 100\np_features = 8\n\nX, y = make_blobs(n_samples = 100, n_features = p_features - 1, centers = [(0, 0), (0, 0)])\n\np = Perceptron()\np.fit(X, y, max_steps = 1000)\nfig = plt.plot(p.history)\nxlab = plt.xlabel(\"Iteration\")\nylab = plt.ylabel(\"Accuracy\")\n\n\n\n\nSimilar to the results from experiment 2, this data set is also not linearly separable because the accuracy never reaches 1.00. It was never able to converge, hence the oscillation of the accuracy scores.\n\n\n\n\n\nThe runtime complexity of a single iteration of the perceptron algorithm update would depend on p, which is the number of features. Finding the \\(\\langle{\\tilde{w}^{(t)},\\tilde{x}_{i}}\\rangle\\) will probably take O(p) time. There are other math operations, however they probably aren’t significant in terms of overall time. Therefore, I think the runtime is O(p)."
  },
  {
    "objectID": "posts/logisticRegression/logisticRegression.html",
    "href": "posts/logisticRegression/logisticRegression.html",
    "title": "My Awesome CSCI 0451 Blog",
    "section": "",
    "text": "Source code: logisticRegression.py\n\n\n\nThis blog post will cover gradient descent and a key variant of the gradient descent called stochastic gradient descent for the logistic regression algorithm. This is an algorithm that uses a logistic function to model the relationship between the input features and the probability of a binary outcome (i.e., “yes” or “no” prediction).\n\n\n\n\nIn our LogisticRegression class, we implemented a fit method and a fit_stochastic method. Both methods take in some input data, X, the target labels, y, as well as hyperparameters alpha and max_epochs. The hyperparameter alpha is the learning rate for the gradient descent algorithm, and max_epochs is the maximum number of iterations the algorithm should run for. The fit_stochastic method also has two additional hyperparameters: batch_size and momentum. The hyperparameters batch_size, is the number of samples to use for each mini-batch during stochastic gradient descent, and momentum is a boolean for whether or not to use momentum in the optimization process.\nIn the fit method, a random weight vector self.w is initialized with shape (p,), where p is the number of features in the modified input data. The method enters a loop that iterates over max_epochs, and with each iteration, it computes the gradient of the loss function with respects to the weight vector by using the gradient method. This method implements the gradient of the empirical risk for logistic regression:\n\nWith each iteration, the method also performs a gradient step to update the weight vector, computes the loss and score on the training data using the updated weight vector, and stores the loss and score histories in self.loss_history and self.score_history. If the difference in loss between consecutive iterations is small, the loop will terminate early.\nThe fit_stochastic method is similar to the fit method, except it computes the gradient descent on batches of the data instead of the whole data. It updates the weight vector with the gradient and momentum, if enabled, accordingly. It then computes the loss and adds it to a list, and checks if the new loss is close enough to the previous loss to terminate early.\n\n\n\n\nBefore starting, you need to load all of these lilbraries and modules:\n\nfrom logisticRegression import LogisticRegression\nfrom sklearn.datasets import make_blobs\nfrom matplotlib import pyplot as plt\nimport numpy as np\nnp.seterr(all='ignore') \n\ndef graph_lossNline(X, y, LR):\n    '''\n    Visualizes the performance of the classifier and check the value of the loss it achieves\n    \n    Parameters:\n        X: Input features of the dataset\n        y: Labels of the dataset\n        LR: Instance of a class with the follow attributes:\n            - w: model parameters\n            - loss_history: empirical risk of the model after each iteration of gradient descent\n            \n    Returns:\n        None\n        \n    Plots:\n        Subplot 1: Scatter plot of the input features, colored according to their labels. The decision boundary of the model is plotted in black\n        Subplot 2: Line plot of the empirical risk of the model as a function of the iteration number\n        \n    Referenced: https://middlebury-csci-0451.github.io/CSCI-0451/lecture-notes/gradient-descent.html\n    '''\n    fig, axarr = plt.subplots(1, 2)\n\n    axarr[0].scatter(X[:,0], X[:,1], c = y)\n    axarr[0].set(xlabel = \"Feature 1\", ylabel = \"Feature 2\", title = f\"Loss = {LR.loss_history[-1]}\")\n\n    f1 = np.linspace(-3, 3, 101)\n\n    p = axarr[0].plot(f1, (- LR.w[2] - f1*LR.w[0])/LR.w[1], color = \"black\")\n\n    axarr[1].plot(LR.loss_history)\n    axarr[1].set(xlabel = \"Iteration number\", ylabel = \"Empirical Risk\")\n    plt.tight_layout()\n\nYou also want to understand how fit(gradient descent) and fit_stochastic (stochastic gradient descent) work. Here is an example for each method:\n\n# make the data\np_features = 3\nX, y = make_blobs(n_samples = 200, n_features = p_features - 1, centers = [(-1, -1), (1, 1)])\n\nfig = plt.scatter(X[:,0], X[:,1], c = y)\nxlab = plt.xlabel(\"Feature 1\")\nylab = plt.ylabel(\"Feature 2\")\n\n\n\n\nHere we generated a data set with two features. Notice how their labels are overlapping each other. We can then fit this data using fit and fit_stochastic with a reasonable learning rate so that they would converge.\n\n# fit the model\n\nLR = LogisticRegression()\nLR.fit(X, y, alpha = 0.1, max_epochs = 1000)\n\ngraph_lossNline(X, y, LR)\n\n\n\n\n\n# fit the model\n\nLR_stochastic = LogisticRegression()\nLR_stochastic.fit_stochastic(X, y, alpha = 0.1, max_epochs = 1000, batch_size = 10)\n\ngraph_lossNline(X, y, LR_stochastic)\n\n\n\n\n\nnum_steps = len(LR_stochastic.loss_history)\nplt.plot(np.arange(num_steps) + 1, LR_stochastic.loss_history, label = \"stochastic gradient\")\n\nnum_steps = len(LR.loss_history)\nplt.plot(np.arange(num_steps) + 1, LR.loss_history, label = \"gradient\")\n\nplt.loglog()\n\nlegend = plt.legend() \n\n\n\n\nIn the visualization above, you can see that the data was able to converge for both methods. The logistic regression with stochastic gradient descent seems to converge faster than logistic regression with regular gradient descent. This may be because stochastic gradient descent updates the weight based on the gradient of the loss function with respect to a small batch of data at a time, while the regular gradient descent is based on the entire training set.\n\n\n\nThis experiment tests the influence of a high learning rate on the convergence of gradient descent and stochastic descent to a minimizer.\n\nLR = LogisticRegression()\nLR.fit(X, y, alpha = 66, max_epochs = 1000)\n\ngraph_lossNline(X, y, LR)\n\n\n\n\n\n# fit the model\n\nLR_stochastic = LogisticRegression()\nLR_stochastic.fit_stochastic(X, y, alpha = 66, max_epochs = 1000, batch_size = 10)\n\ngraph_lossNline(X, y, LR_stochastic)\n\n\n\n\n\nnum_steps = len(LR_stochastic.loss_history)\nplt.plot(np.arange(num_steps) + 1, LR_stochastic.loss_history, label = \"stochastic gradient\")\n\nnum_steps = len(LR.loss_history)\nplt.plot(np.arange(num_steps) + 1, LR.loss_history, label = \"gradient\")\n\nplt.loglog()\n\nlegend = plt.legend() \n\n\n\n\nIt is important to keep the learning rate small to ensure that the algorithm converges stably and the optimization process is under control. If the learning rate is too large, the optimization algorithm can overshoot the minimum of the loss function and fail to converge to an optimal solution.\nYou can see that with increasing the learning rate to 66 from the 0.1 that it was previously set to, the loss function’s values (normal gradient descent) vary significantly instead of decreasing steadily and never converged. The one with stochastic gradient descent failed to converge so badly that it’s loss is not even a number.\n\n\n\n\nThis experiment tests the influence of batch size on how quickly the algorithm converges.\n\n# Make the data\np_features = 19\nX, y = make_blobs(n_samples = 888, n_features = p_features - 1, centers = [(-1, -1), (1, 1)])\n\ndef graph_loss(LR):\n    ''' \n    Plots the loss history of a logistic regression model\n    '''\n    fig, axarr = plt.subplots(1, 1)\n\n    axarr.plot(LR.loss_history)\n    axarr.set(xlabel = \"Iteration number\", ylabel = \"Empirical Risk\", title = f\"Loss = {LR.loss_history[-1]}\")\n    plt.tight_layout()\n\n\nLR_8000 = LogisticRegression()\nLR_8000.fit_stochastic(X, y, alpha = 0.1, max_epochs = 8888, batch_size = 800)\n\ngraph_loss(LR_8000)\n\n\n\n\n\nLR_800 = LogisticRegression()\nLR_800.fit_stochastic(X, y, alpha = 0.1, max_epochs = 8888, batch_size = 80)\n\ngraph_loss(LR_800)\n\n\n\n\n\nLR_80 = LogisticRegression()\nLR_80.fit_stochastic(X, y, alpha = 0.1, max_epochs = 8888, batch_size = 8)\n\ngraph_loss(LR_80)\n\n\n\n\nFor this experiment, we generated a data set that has 888 data points and 18 features. From there, we visualized the loss history of the algorithm if the batch size was 800, 80, and 8. With every decrease in batch size, there was a decrease in the number of iterations done. It is also important to note that when the batch size was 8, there was a bit more fluctuation in the loss than when the batch size was larger. Overall, we can see that a smaller batch size will result in faster convergence, although there will be a bit more variations in loss, and a larger batch size will result in a smoother convergence.\n\n\n\n\nMomentum is a technique used to accerlate the convergence of the optimization algorithm. It adds a fraction of the previous gradient to the current gradient during each iteration. The overall idea is that if the previous weight update was good, keep heading in that direction. The experiment below shows that momentum reduces the number of iterations done.\n\np_features = 3\nX, y = make_blobs(n_samples = 200, n_features = p_features - 1, centers = [(-1, -1), (1, 1)])\n\nfig = plt.scatter(X[:,0], X[:,1], c = y)\nxlab = plt.xlabel(\"Feature 1\")\nylab = plt.ylabel(\"Feature 2\")\n\n\n\n\n\nLR = LogisticRegression()\nLR.fit_stochastic(X, y, alpha = 0.1, max_epochs = 10000, batch_size = 10, momentum = False)\n\ngraph_lossNline(X, y, LR)\n\n\n\n\n\nLR.fit_stochastic(X, y, alpha = 0.1, max_epochs = 10000, batch_size = 10, momentum = True)\n\ngraph_lossNline(X, y, LR)"
  },
  {
    "objectID": "posts/dr-timnit-gebru/dr-timnit-gebru.html",
    "href": "posts/dr-timnit-gebru/dr-timnit-gebru.html",
    "title": "My Awesome CSCI 0451 Blog",
    "section": "",
    "text": "Dr. Gebru’s Talk\nIn her talk at the conference on Computer Vision and Pattern Recogition 2020 as part of a Tutorial on Fairness, Accountability, Transparency, and Ethics (FATE) in Computer Vision, Dr. Gebru provided an overview of the key issues and challenges in developing ethical and fair computer vision systems.\nShe highlighted the ways in chich bias and discrimination can be embedded in the data used to train these systems, as well as the algorithms and models themselves. For example, she noted that if a training data set contains imbalances in representation across different groups, then the resulting model may be diased towards those groupds that are overrepresented in the data. This can lead to unfair outcomes, such as misclassification or exclusion of certain individuals or groups.\nDr. Gebru suggested several approaches for ensuring fairness in computer vision systems, such as using representative data sets, measuring and mitigating bias in algorithms and models, and evaluating these systems on multiple fairness metrics, as different metrics may be more relevant to different applications or contexts. She emphasized the need for greater transparency and accountability in the development and deployment of computer vision systems, especially in high-stakes applications such as criminal justice and healthcare.\nA key takeaway from her talk is that computer vision, as used today, aren’t a neutral or objective technology. These systems can have significant impacts on individuals or society as a whole, particularly in areas such as criminal justice, healthcare, and employment.\n\n\n\nQuestion\nA question for Dr. Gebru is, what steps do you believe are necessary to ensure that the development and deployment of AI systems are guided by ethical principles, and how can these principles be integrated into the broader tech industry culture especially when those who’ve spoken out about these problems have been “punished” by big tech corporations?"
  }
]